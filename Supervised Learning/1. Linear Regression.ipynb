{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee09e325-0eb3-48e8-8dd9-613bc2c45aad",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a fundamental statistical and machine learning technique used to model the relationship between a dependent variable and one or more independent variables. The goal is to find a linear relationship between these variables. Linear regression is a way to understand the relationship between two things by drawing a straight line through data points. It's like finding the best-fitting line through a set of points on a graph. This line helps us predict future values.\n",
    "\n",
    "## Basic Concept\n",
    "\n",
    "1. **Dependent Variable (Target)**: This is what you're trying to predict or explain (e.g., house prices).\n",
    "2. **Independent Variables (Features)**: These are the variables you're using to predict the dependent variable (e.g., size of the house, number of bedrooms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b582a-e409-4f72-87ab-91d299ab4878",
   "metadata": {},
   "source": [
    "# Assumptions in Linear Regression\n",
    "\n",
    "Linear regression is a powerful tool for predicting a dependent variable based on independent variables. However, for it to be effective, certain assumptions must be met. Understanding and checking these assumptions is crucial for a valid regression model.\n",
    "\n",
    "## Why Assumptions Matter\n",
    "\n",
    "Meeting these assumptions is crucial for the linear regression model to provide accurate and reliable predictions. Violating these assumptions can lead to biased estimates, incorrect conclusions, and poor model performance.\n",
    "\n",
    "## Key Assumptions\n",
    "\n",
    "### 1. Linearity\n",
    "- **What It Is**: The relationship between the independent and dependent variables should be linear.\n",
    "- **Importance**: Non-linear relationships cannot be accurately captured by a linear model.\n",
    "\n",
    "### 2. Independence / No autocorrelation\n",
    "- **What It Is**: Observations should be independent of each other.\n",
    "- **Importance**: Correlated observations can lead to unreliable and unstable estimates of regression coefficients.\n",
    "\n",
    "### 3. Homoscedasticity\n",
    "- **What It Is**: The residuals (errors) should have constant variance.\n",
    "- **Importance**: If the variance of residuals changes, it can affect the reliability of the model's forecasts.\n",
    "\n",
    "### 4. Normal Distribution of Errors\n",
    "- **What It Is**: The error terms should be normally distributed.\n",
    "- **Importance**: This assumption allows for the derivation of confidence intervals and hypothesis tests. CLT, T test, f test works because we assumed normality.\n",
    "\n",
    "### 5. No or Little Multicollinearity\n",
    "- **What It Is**: Independent variables should not be too highly correlated with each other.\n",
    "- **Importance**: High multicollinearity can make it difficult to determine the individual effect of independent variables.\n",
    "\n",
    "### 6. No Endogeneity\n",
    "- **What It Is**: What you're studying is actually causing the effect you're seeing, and not something else you hadn’t thought of\n",
    "- **Importance**: if you're making decisions based on your study, you want to be sure that your conclusions are accurate\n",
    "\n",
    "\n",
    "Laymen\n",
    "### 1. Linearity\r\n",
    "- **What It Is**:The relationship between the independent and dependent variables should be linear. This means that changes in the independent variable (like hours of study) will result in proportional changes in the dependent variable (like exam scores) in a straight-line manner.\r\n",
    "- **Importance**: If the relationship is not linear, linear models won't be able to accurately capture and predict the dependent variable. This can lead to incorrect conclusions.\r\n",
    "\r\n",
    "### 2. Independence / No autocorrelation\r\n",
    "- **What It Is**: Each observation (like a customer's purchase decision) should be independent of others. In other words, one observation should not influence or predict another.\r\n",
    "- **Importance**: When observations are correlated, it can result in unreliable estimates, as the model will interpret these patterns as significant when they might not be.\r\n",
    "\r\n",
    "### 3. Homoscedasticity\r\n",
    "- **What It Is**: The spread of the residuals (errors) around the regression line should be constant. It means the uncertainty or error is the same across all levels of the independent variable.\r\n",
    "- **Importance**: Variable spread (heteroscedasticity) can lead to inefficient estimates and affect the accuracy of predictions, especially for values at the extremes.\r\n",
    "\r\n",
    "### 4. Normal Distribution of Errors\r\n",
    "- **What It Is**: The residuals (or errors) of the model should follow a normal distribution, forming a bell-shaped curve when plotted.\r\n",
    "- **Importance**: This assumption allows for more reliable statistical inferences, enabling the use of various tests and confidence measures that presume normality.\r\n",
    "\r\n",
    "### 5. No or Little Multicollinearity\r\n",
    "- **What It Is**: The independent variables in the model should not be too highly correlated with each other. Each variable should provide unique information.\r\n",
    "- **Importance**: High multicollinearity can obscure the individual effect of each variable, making it difficult to understand how each one is influencing the dependent variable.\r\n",
    "\r\n",
    "### 6. No Endogeneity\r\n",
    "- **What It Is**: The causal relationships assumed in the model should be accurate. The independent variables should cause the changes in the dependent variable, not the other way around or due to some omitted variable.\r\n",
    "- **Importance**: Incorrect assumptions about causality can lead to erroneous conclusions and ineffective solutions based on linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c975d-aa61-439f-b084-669dc0ee3ebf",
   "metadata": {},
   "source": [
    "### 1. Linearity\r\n",
    "- **How to Check**: \r\n",
    "  - Look at graphs of your data. If the relationship between what you're studying (like hours of study) and what you're measuring (like test scores) looks like a straight line, you're good.\r\n",
    "  - Use special plots (partial regression plots) to see how each thing you're studying affects what you're measuring, one at a time.\r\n",
    "\r\n",
    "- **How to Fix**: \r\n",
    "  - If the relationship isn’t a straight line, try transforming the data (like using logs or square roots) or consider using a model that handles curves.\r\n",
    "\r\n",
    "### 2. Independence / No autocorrelation\r\n",
    "- **How to Check**: \r\n",
    "  - There's a statistic called Durbin-Watson that helps test Durbin-Watson falls between 0 and 4. 2 is no auto-correlation, <1 and>3 are cause of alarm.  Also, plotting the residuals (the differences between what the model predicts and what you actually see) over time can show patterns. patterns.\r\n",
    "  \r\n",
    "- **How to Fix**: \r\n",
    "  - Make sure each data point you collect doesn't depend on the previous ones. For time-related data, use special time s (like ARIMA models) that account for autocorrelation.ries methods.\r\n",
    "\r\n",
    "### 3. Homoscedasticity\r\n",
    "- **How to Check**: \r\n",
    "  - Plot the residuals against the predicted values. The spread should look the same all across the plot.\r\n",
    "\r\n",
    "- **How to Fix**: \r\n",
    "  - Transform your data or try different kinds of regression that give different weights to different points.\r\n",
    "\r\n",
    "### 4. Normal Distribution of Errors\r\n",
    "- **How to Check**: \r\n",
    "  - Use a Q-Q plot to compare your residuals to a perfect bell curve. There are also formal tests like Shapiro-Wilk.\r\n",
    "  \r\n",
    "- **How to Fix**: \r\n",
    "  - Transforming your data can help. If you have a lot of data, this might not be as big of an issue thanks to the Central Limit Theorem.\r\n",
    "\r\n",
    "### 5. No or Little Multicollinearity\r\n",
    "- **How to Check**: \r\n",
    "  - Calculate something called Variance Inflation FacVIF values greater than 5 or 10 indicate problematic multicollinearity.tors (VIF) for your variables. Also, check how much your variables are related to each other.\r\n",
    "  \r\n",
    "- **How to Fix**: \r\n",
    "  - You might need to remove some variables that are too similar or combine them in some way. Techniques like Principal Component Analysis (PCA) can also help.\r\n",
    "\r\n",
    "### 6. No Endogeneity\r\n",
    "- **How to Check**: \r\n",
    "  - Think about your study design and whether something else might be causing your results. There are also methods like using instrumental variables to test this.\r\n",
    "  \r\n",
    "- **How to Fix**: \r\n",
    "  - Use variables that are related to your main variables but not directly related to your results. Improving your study dinary least squares regression analysis.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "While linear regression is a versatile and straightforward modeling technique, ensuring that these key assumptions are met is essential for effective and valid model outcomes. Each assumption violation has specific remedies. It's vital to understand the nature of your data and apply the appropriate methods to ensure the validity of your linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2a7c-ea14-4350-b9ff-47eb84bd3575",
   "metadata": {},
   "source": [
    "\n",
    "## The Linear Equation\n",
    "\n",
    "Linear regression models this relationship with a linear equation, which in its simplest form (with one independent variable) is:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X + \\epsilon \\]\n",
    "\n",
    "- `Y` is the dependent variable.\n",
    "- `X` is the independent variable.\n",
    "- `\\beta_0` is the y-intercept (the value of `Y` when `X = 0`).\n",
    "- `\\beta_1` is the slope of the line (how much `Y` changes for a unit change in `X`).\n",
    "- `\\epsilon` is the error term, accounting for the fact that the relationship isn't perfectly linear.\n",
    "\n",
    "# Simple Linear Regression Equation Explained\r\n",
    "\r\n",
    "Simple Linear Regression is a way to show the relationship between two things using a straight line. It's like finding the best straight path through a series of points on a graph. This line helps us predict how one thing changes when another thing changes.\r\n",
    "\r\n",
    "## Understanding the Equation\r\n",
    "\r\n",
    "The equation for simple linear regression is:\r\n",
    "\r\n",
    "`Y = a + bX`\r\n",
    "\r\n",
    "This might look a bit technical, but it's actually quite straightforward when you break it down:\r\n",
    "\r\n",
    "- `Y`: This is what we want to predict or understand better. For example, it could be the price of a house.\r\n",
    "- `X`: This is what we think affects `Y`. In our house price example, this could be the size of the house.\r\n",
    "- `a`: This is where the line crosses the Y-axis when `X` is zero. It's like the starting point of our line if `X` had no effect.\r\n",
    "- `b`: This shows how much `Y` changes when `X` changes. If `b` is positive, it means that as `X` increases, `Y` also increases. In our example, a larger house size would mean a higher price.\r\n",
    "\r\n",
    "## A Simple Example\r\n",
    "\r\n",
    "Imagine we want to understand how the number of hours spent studying affects a student's test score:\r\n",
    "\r\n",
    "- `Y` (what we want to predict): Test score\r\n",
    "- `X` (what we think affects the score): Hours spent studying\r\n",
    "- `a`: The score a student might get if they didn't study at all\r\n",
    "- `b`: How much the score is expected to increase for each additional hour of study\r\n",
    "\r\n",
    "If our equation is `Y = 10 + 5X`, it means that if a student doesn't study at all (`X=0`), the expected score would be 10 (`Y=10`). For each hour spent studying, the score increases by 5 points.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "The simple linear regression equation is a basic but powerful tool to understand and predict how two things are related. It helps us draw a line through data points on a graph, showing the average effect of one thing on another.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bb794-783e-4c76-986f-0ef3027d5563",
   "metadata": {},
   "source": [
    "## Example: Study Hours and Exam Marksion?\r\n",
    "\r\n",
    "Imagine you're trying to figure out if there's a relationship between the number of hours you study and the marks you get in an exam. In this case, the number of hours studied is what you control (independent variable), and the marks you get is what you want to predict (dependent variam Marks\r\n",
    "\r\n",
    "Let's say we plot the study hours and exam marks of different students on a graph:\r\n",
    "\r\n",
    "- The **horizontal axis (X-axis)** shows the study hours.\r\n",
    "- The **vertical axis (Y-axis)** shows the exam marks.\r\n",
    "\r\n",
    "Each point on this graph represents a student's study hours and their corresponding exam marks.\r\n",
    "\r\n",
    "## Finding the Best-Fitting Line\r\n",
    "\r\n",
    "Linear regression helps us draw a straight line through these points. This line represents the average effect of studying for a certain number of hours on the exam marks. The goal is to draw this line so that it's as close as possible to all the points.\r\n",
    "\r\n",
    "### How Does This Line Help?\r\n",
    "\r\n",
    "1. **Prediction**: If you know how many hours a student plans to study, you can use the line to predict their exam marks.\r\n",
    "2. **Understanding Relationship**: The line also shows the relationship between study hours and marks. If the line goes up as it moves from left to right, it means more study hours generally lead to higher marks.\r\n",
    "\r\n",
    "## Real-World Example\r\n",
    "\r\n",
    "Think about a real estate agent trying to price a house. They might use linear regression to understand the relationship between the house’s size (in square feet) and its selling price. Here, the size of the house is the independent variable, and the selling price is the dependent variable.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "In summary, linear regression is a way to understand how two things are related. It's like drawing the best line through a scatter of dots on a graph to predict and understand how changing one thing (like study hours or house size) might affect another thing (like exam marks or selling price).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe13f0-0ada-42ac-a64e-3cd5aea04261",
   "metadata": {},
   "source": [
    "# Difference Between Correlation and Linear Regression\r\n",
    "\r\n",
    "Understanding data often involves looking at the relationship between variables. Two common methods to do this are correlation and linear regression. While they may seem similar, they serve different purposes and convey different types of information.\r\n",
    "\r\n",
    "## Correlation\r\n",
    "\r\n",
    "Correlation measures the strength and direction of the linear relationship between two variables. It's a statistical technique that tells us how closely variables move together.\r\n",
    "\r\n",
    "### Key Points:\r\n",
    "\r\n",
    "- **Scale**: The correlation coefficient ranges from -1 to 1. A value close to 1 means a strong positive relationship, -1 means a strong negative relationship, and 0 means no linear relationship.\r\n",
    "- **Direction**: Indicates whether the variables increase/decrease together (positive correlation) or move in opposite directions (negative correlation).\r\n",
    "- **No Distinction**: Treats both variables equally; doesn’t distinguish between dependent and independent variables.\r\n",
    "- **Purpose**: Mainly used to quantify the degree of association between variables.\r\n",
    "\r\n",
    "## Linear Regression\r\n",
    "\r\n",
    "Linear regression, on the other hand, is used to predict the value of a dependent variable based on the value of at least one independent variable. It explains the impact of changes in an independent variable on the dependent variable.\r\n",
    "\r\n",
    "### Key Points:\r\n",
    "\r\n",
    "- **Equation**: Uses the equation `Y = a + bX`, where `Y` is the dependent variable, `X` is the independent variable, `a` is the intercept, and `b` is the slope.\r\n",
    "- **Predictive**: Focuses on the relationship and predicts future outcomes.\r\n",
    "- **Causality Direction**: Implies a directional effect (X influences Y).\r\n",
    "- **Purpose**: Used to understand and predict the behavior of one variable based on the behavior of another.\r\n",
    "\r\n",
    "## Comparison\r\n",
    "\r\n",
    "| Aspect         | Correlation         | Linear Regression  |\r\n",
    "| -------------- | ------------------- | ------------------ |\r\n",
    "| Purpose        | Measures the strength and direction of a linear relationship. | Predicts and explains the relationship between variables. |\r\n",
    "| Directionality | Bidirectional; doesn’t imply cause and effect. | Unidirectional; implies a predictive relationship from independent to dependent variable. |\r\n",
    "| Output         | Correlation coefficient (a single number). | Equation that describes the line of best fit. |\r\n",
    "| Application    | Used when simply understanding the relationship is the goal. | Used when the goal is to predict or explain changes in one variable due to another. |\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "In summary, while correlation and linear regression may seem similar as they both deal with relationships between variables, they serve different purposes. Correlation quantifies the strength of a relationship, whereas linear regression provides a model to predict and explain changes in variables.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1546fa95-e692-4acc-82c4-c6b725e589e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.600\n",
      "Model:                            OLS   Adj. R-squared:                  0.467\n",
      "Method:                 Least Squares   F-statistic:                     4.500\n",
      "Date:                Fri, 05 Jan 2024   Prob (F-statistic):              0.124\n",
      "Time:                        21:06:17   Log-Likelihood:                -5.2598\n",
      "No. Observations:                   5   AIC:                             14.52\n",
      "Df Residuals:                       3   BIC:                             13.74\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.2000      0.938      2.345      0.101      -0.785       5.185\n",
      "X              0.6000      0.283      2.121      0.124      -0.300       1.500\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   2.017\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.570\n",
      "Skew:                           0.289   Prob(JB):                        0.752\n",
      "Kurtosis:                       1.450   Cond. No.                         8.37\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neurabytes-taranjit\\.virtualenvs\\datascience_practice_machinelearning-Sjf4ONJ-\\Lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data - replace this with your actual dataset\n",
    "data = {\n",
    "    'X': [1, 2, 3, 4, 5],  # Independent variable\n",
    "    'Y': [2, 4, 5, 4, 5]   # Dependent variable\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Defining the independent and dependent variables\n",
    "X = df['X']\n",
    "y = df['Y']\n",
    "\n",
    "# Adding a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fitting the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Printing the regression table\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a847f5-2811-48f0-89a0-9c2f73203f50",
   "metadata": {},
   "source": [
    "# How to Read Regression Tables\r\n",
    "\r\n",
    "Understanding regression tables is crucial in interpreting the results of statistical analysis, especially in fields like economics, social sciences, and various branches of science. Here's a guide on how to read these tables.\r\n",
    "\r\n",
    "## Components of a Regression Table\r\n",
    "\r\n",
    "A typical regression table includes several key components:\r\n",
    "\r\n",
    "### 1. Coefficients\r\n",
    "- **What they are**: These numbers represent the estimated effect of each independent variable on the dependent variable.\r\n",
    "- **How to interpret**: A positive coefficient suggests a positive relationship, while a negative coefficient indicates an inverse relationship.\r\n",
    "\r\n",
    "### 2. Standard Errors\r\n",
    "- **What they are**: This indicates the standard deviation of the estimated coefficients.\r\n",
    "- **How to interpret**: Smaller standard errors suggest more precise estimates.\r\n",
    "\r\n",
    "### 3. t-Statistics\r\n",
    "- **What they are**: These are calculated by dividing the coefficient by its standard error.\r\n",
    "- **How to interpret**: Used to determine the significance of each coefficient.\r\n",
    "\r\n",
    "### 4. P-values\r\n",
    "- **What they are**: These values give the probability of observing the data if the null hypothesis (typically, that there is no relationship) is true.\r\n",
    "- **How to interpret**: A small p-value (usually < 0.05) suggests that the null hypothesis can be rejected, indicating a significant effect.\r\n",
    "\r\n",
    "### 5. R-squared\r\n",
    "- **What it is**: This is a measure of how well the independent variables explain the variability in the dependent variable.\r\n",
    "- **How to interpret**: Values range from 0 to 1, with higher values indicating a better fit.\r\n",
    "\r\n",
    "### 6. F-Statistic\r\n",
    "- **What it is**: This tests the overall significance of the model.\r\n",
    "- **How to interpret**: Like the p-value, a low value suggests the model is statistically significant.\r\n",
    "\r\n",
    "### 7. Degrees of Freedom\r\n",
    "- **What they are**: This represents the number of independent data points minus the number of estimated parameters.\r\n",
    "- **How to interpret**: Used in calculating the standard error and the t-statistics.\r\n",
    "\r\n",
    "### 8. Confidence Interval\r\n",
    "- **What it is**: This range of values is likely to include the true value of the coefficient.\r\n",
    "- **How to interpret**: Wider intervals indicate less precision, while narrower intervals suggest greater precision.\r\n",
    "\r\n",
    "## Example of Reading a Table\r\n",
    "\r\n",
    "Consider a regression table with a coefficient of 2.0 for an independent variable, a standard error of 0.5, and a p-value of 0.01. This suggests that the variable has a significant positive effect on the dependent variable, and we can be confident about this finding due to the low p-value and relatively small standard error.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "Regression tables provide a wealth of information about the relationships between variables. Understanding how to read these tables is essential for interpreting the results of statistical analyses accurately.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c6967-aa10-4587-bcfd-a5736d8d1e98",
   "metadata": {},
   "source": [
    "# Decomposition of Variability in Statistical Analysis\n",
    "\n",
    "Understanding the decomposition of variability is essential in regression analysis, as it helps in evaluating the performance of the model. This concept involves breaking down the total variation in a dataset into component parts.\n",
    "\n",
    "## Total Variation\n",
    "\n",
    "Total variation measures the overall spread of the data points in your dataset. It's a key starting point for understanding data variability.\n",
    "\n",
    "- **Formula**: Total Variation = Σ(yᵢ - ȳ)²\n",
    "- **Where**: \n",
    "  - `yᵢ` is an individual data point\n",
    "  - `ȳ` is the mean of all data points\n",
    "\n",
    "## Decomposition in Regression Analysis\n",
    "\n",
    "In regression analysis, total variation is decomposed into two main components:\n",
    "\n",
    "### 1. Explained Variation\n",
    "\n",
    "Explained variation is the part of the total variation that the independent variables in the model explain.\n",
    "\n",
    "- **Formula**: Explained Variation = Σ(ŷᵢ - ȳ)²\n",
    "- **Where**: \n",
    "  - `ŷᵢ` is the predicted value from the regression model\n",
    "\n",
    "### 2. Unexplained Variation (Residuals)\n",
    "\n",
    "Unexplained variation (or residuals) is the portion of the total variation that the model fails to explain.\n",
    "\n",
    "- **Formula**: Unexplained Variation = Σ(yᵢ - ŷᵢ)²\n",
    "- **Where**: \n",
    "  - `yᵢ` is the actual value\n",
    "  - `ŷᵢ` is the predicted value\n",
    "\n",
    "## Importance in Regression Analysis\n",
    "\n",
    "- **Model Evaluation**: Decomposing variability helps evaluate the performance of a regression model.\n",
    "- **R-squared Statistic**: This decomposition forms the basis of the R-squared statistic, a key measure of model fit.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Decomposition of variability is a fundamental aspect of regression analysis. It provides insight into how well a model captures the patterns in the data and guides improvements in model selection and feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879233a7-214f-4894-9596-e45baf6795b1",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) Explained\r\n",
    "\r\n",
    "Ordinary Least Squares (OLS) is a fundamental method in statistical modeling, particularly in linear regression. It is used to estimate the parameters in a linear regression modeLS\r\n",
    "\r\n",
    "OLS aims to find the line that best fits a set of data points by minimizing the sum of the squares of the vertical distances of the points from the \r\n",
    "Ordinary Least Squares (OLS) is a method used in linear regression for estimating the unknown parameters in a linear regression model. OLS does this by minimizing the sum of the squares of the differences between the observed dependent variable and those predicted by the linear function. In simpler terms, it tries to find the best-fitting line through the data points by minimizing the vertical distances of the points from the line. line.\r\n",
    "\r\n",
    "### Key Concepts\r\n",
    "\r\n",
    "- **Best Fit**: OLS identifies the line that minimizes the discrepancy between observed values and values predicted by the model.\r\n",
    "- **Least Squares**: The method minimizes the sum of the squares of the differences between observed and predicted values.\r\n",
    "\r\n",
    "## The OLS Equation\r\n",
    "\r\n",
    "In a simple linear regression model `Y = β₀ + β₁X + ε`, OLS helps in estimating the coefficients (β₀ and β₁) that best describe the relationship between dependent variable `Y` and independent variable `X`.\r\n",
    "\r\n",
    "## Process\r\n",
    "\r\n",
    "1. **Model Specification**: Defining the linear relationship between the variables.\r\n",
    "2. **Parameter Estimation**: Using OLS to estimate the model parameters that minimize the sum of squared residuals.\r\n",
    "3. **Model Evaluation**: Evaluating the model's effectiveness using various statistics like R-squared, t-tests, F-tests, etc.\r\n",
    "\r\n",
    "## Assumptions of OLS\r\n",
    "\r\n",
    "For OLS estimates to be optimal, certain assumptions must be met:\r\n",
    "\r\n",
    "1. **Linearity**: The relationship between dependent and independent variables should be linear.\r\n",
    "2. **Independence**: Observations should be independent of each other.\r\n",
    "3. **Homoscedasticity**: The residuals should have constant variance.\r\n",
    "4. **No Autocorrelation**: The residuals should not be correlated.\r\n",
    "5. **Normal Distribution of Errors**: Ideally, the residuals should be normally distributed.\r\n",
    "\r\n",
    "## Advantages and Limitations\r\n",
    "\r\n",
    "### Advantages\r\n",
    "\r\n",
    "- **Simplicity**: OLS is straightforward to understand and implement.\r\n",
    "- **Efficiency**: In the presence of the above assumptions, OLS provides the most efficient (unbiased with the smallest variance) estimates.\r\n",
    "\r\n",
    "### Limitations\r\n",
    "\r\n",
    "- **Assumption-Dependent**: If the assumptions of OLS are violated, the estimates may be inefficient, biased, or inconsistent.\r\n",
    "- **Not Robust to Outliers**: OLS is sensitive to outliers which can significantly impact the regression line.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "OLS is a crucial technique in statistics and econometrics, forming the foundation for many other methods in data analysis. Its simplicity and efficiency make it a popular choice, but careful attention must be paid to its assumptions.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18cb11e-185c-4708-bd49-7571fdd1c00b",
   "metadata": {},
   "source": [
    "# Understanding R-squared in Regression Analysis\r\n",
    "\r\n",
    "R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\r\n",
    "\r\n",
    "## What is R-squared?\r\n",
    "\r\n",
    "- **Definition**: R-squared, also known as the coefficient of determination, is a key output of regression analysis.\r\n",
    "- **Range**: It ranges from 0 to 1.\r\n",
    "- **Interpretation**: An R-squared of 0 means that the dependent variable cannot be predicted from the independent variable(s); a value of 1 means the dependent variable can be predicted without error from the independent variable(s).\r\n",
    "\r\n",
    "## Formula\r\n",
    "\r\n",
    "The formula for R-squared is:\r\n",
    "\r\n",
    "`R² = 1 - (Sum of Squares of Residuals / Total Sum of Squares)`\r\n",
    "\r\n",
    "- **Sum of Squares of Residuals**: Variability left unexplained after performing the regression.\r\n",
    "- **Total Sum of Squares**: Total variability in the dependent variable.\r\n",
    "\r\n",
    "## Significance\r\n",
    "\r\n",
    "- **Measure of Fit**: Indicates how well the data fit a regression model (the higher the R-squared, the better the model fits your data).\r\n",
    "- **Not a Complete Measure**: A high R-squared does not necessarily mean the model is good. It does not indicate whether the regression model is adequate, nor whether it is biased.\r\n",
    "\r\n",
    "## Limitations\r\n",
    "\r\n",
    "- **Doesn’t Indicate Causality**: A high R-squared doesn’t imply a causal relationship between variables.\r\n",
    "- **Sensitive to Overfitting**: Adding more predictors to a model can artificially inflate the R-squared value, even if the predictors are irrelevant.\r\n",
    "- **Not Suitable for Comparing Models with Different Numbers of Predictors**: It can be misleading when comparing models with different numbers of independent variables.\r\n",
    "\r\n",
    "## Adjusted R-squared\r\n",
    "\r\n",
    "- **What It Is**: A modified version of R-squared that has been adjusted for the number of predictors in the model.\r\n",
    "- **Purpose**: Addresses the issue of the R-squared increasing with the addition of variables, regardless of their usefulness.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "While R-squared is a useful indicator of how well your model fits the data, it should be used in conjunction with other metrics and tests to ensure the model's adequacy, reliability, and validity.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee5641-7cac-487b-a737-321caaa33024",
   "metadata": {},
   "source": [
    "\n",
    "## Assumptions\n",
    "\n",
    "Linear regression relies on several key assumptions:\n",
    "   \n",
    "- **Linearity**: The relationship between the independent and dependent variables should be linear.\n",
    "- **Independence**: Observations should be independent of each other.\n",
    "- **Homoscedasticity**: The residuals (difference between observed and predicted values) should have constant variance.\n",
    "- **Normal Distribution of Errors**: The residuals should be normally distributed.\n",
    "\n",
    "\n",
    "## Fitting the Model\n",
    "\n",
    "- **Least Squares Method**: This is the most common method used to estimate the coefficients (`\\beta`) of the linear regression model. It minimizes the sum of the squared differences between observed and predicted values.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "- **R-squared**: Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Adjusted R-squared**: Adjusted for the number of predictors in the model, used for multiple linear regression.\n",
    "- **Residual Analysis**: Assessing the residuals (errors) to check if they meet the assumptions.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Linear regression is used in various fields like economics (predicting GDP), finance (stock prices), biology (drug response), and many more.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Cannot model non-linear relationships.\n",
    "- Sensitive to outliers.\n",
    "- Assumes a linear relationship between variables and constant variance.\n",
    "\n",
    "In summary, linear regression is a starting point for regression analysis. It's straightforward to understand and implement but has limitations, especially when dealing with non-linear data or outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b746a-3473-4a4c-aee8-8f8d604c9828",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Explained\r\n",
    "\r\n",
    "Multiple Linear Regression is an extension of Simple Linear Regression and is used to model the relationship between two or more independent variables and a single dependent variable.\r\n",
    "\r\n",
    "## What is Multiple Linear Regression?\r\n",
    "\r\n",
    "- **Definition**: Multiple Linear Regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable.\r\n",
    "- **Goal**: The goal is to model the linear relationship between the dependent (Y) and independent variables (X₁, X₂, ..., Xₙ).\r\n",
    "\r\n",
    "## The MLR Equation\r\n",
    "\r\n",
    "The equation for a multiple linear regression model is:\r\n",
    "\r\n",
    "`Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε`\r\n",
    "\r\n",
    "- `Y`: Dependent variable (what you want to predict)\r\n",
    "- `β₀`: Y-intercept (constant term)\r\n",
    "- `β₁, β₂, ..., βₙ`: Coefficients of independent variables\r\n",
    "- `X₁, X₂, ..., Xₙ`: Independent variables\r\n",
    "- `ε`: Error of the estimate\r\n",
    "\r\n",
    "## Key Concepts\r\n",
    "\r\n",
    "- **Multivariable Analysis**: MLR analyzes the effect of multiple variables on a single response variable.\r\n",
    "- **Coefficients (β₁, β₂, ..., βₙ)**: Represent the change in the dependent variable for one unit change in an independent variable, assuming other variables are held constant.\r\n",
    "\r\n",
    "## Assumptions\r\n",
    "\r\n",
    "For MLR to be effective, certain assumptions must be met:\r\n",
    "\r\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable should be linear.\r\n",
    "2. **No Multicollinearity**: Independent variables should not be too highly correlated with each other.\r\n",
    "3. **Homoscedasticity**: The residuals (differences between observed and predicted values) should have constant variance.\r\n",
    "4. **Independence**: Observations should be independent of each other.\r\n",
    "5. **Normal Distribution of Residuals**: The residuals should be normally distributed.\r\n",
    "\r\n",
    "## Applications\r\n",
    "\r\n",
    "Multiple Linear Regression is widely used in various fields such as economics, business, engineering, and the social sciences for:\r\n",
    "\r\n",
    "- Predicting outcomes (e.g., sales, revenues)\r\n",
    "- Analyzing the impact of price changes\r\n",
    "- Assessing risk factors in finance and healthcare\r\n",
    "\r\n",
    "## Limitations\r\n",
    "\r\n",
    "- **Overfitting**: Including irrelevant variables can make the model overly complex.\r\n",
    "- **Underfitting**: Excluding relevant variables can lead to a poor model fit.\r\n",
    "- **Causality**: MLR does not imply causation, even if the model fits well.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "Multiple Linear Regression is a powerful tool for predictive modeling and analysis. However, the correct application of MLR requires careful consideration of its assumptions and an understanding of the underlying data.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8fb23-8ed8-4a9b-8e7c-7f6f8b5d0de8",
   "metadata": {},
   "source": [
    "# Adjusted R-squared in Regression Analysis\r\n",
    "\r\n",
    "Adjusted R-squared is a statistical measure that modifies the R-squared value for the number of predictors in a regression model. It's particularly useful in the context of multiple linear regression.\r\n",
    "\r\n",
    "## What is Adjusted R-squared?\r\n",
    "\r\n",
    "- **Definition**: Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model.\r\n",
    "- **Purpose**: It provides a more accurate measure of the goodness of fit, especially when comparing models with different numbers of independent variables.\r\n",
    "\r\n",
    "## Formula\r\n",
    "\r\n",
    "The formula for Adjusted R-squared is:\r\n",
    "\r\n",
    "`Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]`\r\n",
    "\r\n",
    "- `R²`: The R-squared value.\r\n",
    "- `n`: The total number of observations.\r\n",
    "- `k`: The number of independent variables.\r\n",
    "\r\n",
    "## Key Differences from R-squared\r\n",
    "\r\n",
    "- **Penalizes Complexity**: Unlike R-squared, Adjusted R-squared decreases if additional predictors do not improve the model significantly.\r\n",
    "- **Comparing Models**: More reliable than R-squared for comparing models with different numbers of independent variables.\r\n",
    "\r\n",
    "## When to Use Adjusted R-squared\r\n",
    "\r\n",
    "- **Multiple Regression Models**: Particularly useful when you have multiple predictors and need to assess the contribution of each.\r\n",
    "- **Model Selection**: Helps in selecting the right combination of variables by penalizing the addition of irrelevant predictors.\r\n",
    "\r\n",
    "## Limitations\r\n",
    "\r\n",
    "- **Not a Definitive Measure**: A higher Adjusted R-squared does not always mean a better model. Other model assumptions and diagnostics should be considered.\r\n",
    "- **Not Applicable to All Models**: Adjusted R-squared is most meaningful in the context of linear regression models.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "While R-squared gives a quick indication of how well a model fits the data, Adjusted R-squared provides a more nuanced picture by adjusting for the number of predictors. It's an essential tool in the model evaluation process, helping to balance model complexity and fit.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffd0e9-9d61-40ce-a416-0e6e3ddb2684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "381e067a-4053-4628-b32f-6b53fbbc8de6",
   "metadata": {},
   "source": [
    "### Dummy Variables in Linear Regression\r\n",
    "\r\n",
    "- **What Are Dummy Variables**:\r\n",
    "  - Dummy variables are used in linear regression to represent categorical data. They are binary (0 or 1) variables created to include attributes like gender, color, brand, etc., which are not numerical.\r\n",
    "  - For example, in a dataset, 'Gender' can be represented as a dummy variable where 'Male' is 1 and 'Female' is 0. \r\n",
    "\r\n",
    "- **Why Use Dummy Variables**:\r\n",
    "  - Linear regression models require numerical inputs, but often, data includes categorical (non-numerical) information. Dummy variables convert this categorical data into a numerical format that can be used in the regression model.\r\n",
    "  - They allow the model to correctly interpret the categories without assuming a natural ordering (like one category being higher or lower than another).\r\n",
    "\r\n",
    "- **How to Create Dummy Variables**:\r\n",
    "  - Identify categorical variables in your dataset that need to be included in the regression.\r\n",
    "  - For each category within a variable, create a new dummy variable.\r\n",
    "  - Assign a value of 1 or 0 to these dummy variables. For instance, if you have a variable for 'Color' with categories 'Red', 'Blue', and 'Green', you can create two dummies: one for 'Red' (1 if Red, 0 otherwise) and one for 'Blue' (1 if Blue, 0 otherwise). 'Green' can be implied if both dummies are 0.\r\n",
    "\r\n",
    "- **Things to Keep in Mind**:\r\n",
    "  - **Avoiding the Dummy Variable Trap**: This occurs when dummy variables are highly correlated (multicollinear). To avoid this, always omit one dummy variable for each categorical variable. This omitted category serves as the baseline against which the others are compared.\r\n",
    "  - **Interpreting Regression Coefficients**: The coefficients of dummy variables represent the change in the response variable when the dummy is 1 compared to when it is 0 (or compared to the baseline category, if one dummy is omitted).\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4fb78-2d51-406c-96ea-a8d640d22d94",
   "metadata": {},
   "source": [
    "# Use Cases of Simple Linear Regression in Customer Acquisition in Finance\r\n",
    "\r\n",
    "Simple linear regression is a powerful tool in finance, particularly for strategies related to customer acquisition. Below are several key applications:\r\n",
    "\r\n",
    "## 1. Predicting Customer Lifetime Value (CLV)\r\n",
    "- **Objective**: Estimate the potential lifetime value of new customers.\r\n",
    "- **Application**: Financial institutions can use simple linear regression to predict a customer's lifetime value based on initial metrics like deposit amounts or credit scores. This aids in identifying high-value prospects.\r\n",
    "\r\n",
    "## 2. Credit Scoring Models\r\n",
    "- **Objective**: Assess the creditworthiness of new loan applicants.\r\n",
    "- **Application**: Regression analysis can predict loan repayment likelihood based on an applicant's credit score, aiding in risk assessment.\r\n",
    "\r\n",
    "## 3. Response Modeling for Marketing Campaigns\r\n",
    "- **Objective**: Predict customer responses to marketing campaigns.\r\n",
    "- **Application**: Understanding the influence of marketing strategies on customer acquisition through regression helps optimize marketing efforts.\r\n",
    "\r\n",
    "## 4. Analyzing the Effect of Interest Rates on New Account Openings\r\n",
    "- **Objective**: Understand the impact of interest rate changes on new account openings.\r\n",
    "- **Application**: Predicting new account sign-ups in response to interest rate changes assists in strategic rate adjustments.\r\n",
    "\r\n",
    "## 5. Predicting the Success of Referral Programs\r\n",
    "- **Objective**: Evaluate the effectiveness of customer referral programs.\r\n",
    "- **Application**: Linear regression can relate referral program incentives to the number of successful new customer acquisitions.\r\n",
    "\r\n",
    "## 6. Investment Product Sales Forecasting\r\n",
    "- **Objective**: Forecast sales of various investment products.\r\n",
    "- **Application**: Advisors can predict sales of investment products based on market trends and customer demographics.\r\n",
    "\r\n",
    "## 7. Risk Assessment for Customer Segmentation\r\n",
    "- **Objective**: Segment customers based on risk profiles.\r\n",
    "- **Application**: Categorizing customers into risk segments based on income, investment history, etc., for targeted product offerings.\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "Simple linear regression in the finance sector is invaluable for understanding and optimizing customer acquisition strategies. It helps in tailoring financial products and services to meet diverse customer needs effectively.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047b5f6-264f-400c-b4ac-eb76d3ab12ae",
   "metadata": {},
   "source": [
    "# Comparison of Regression Types and Their Suitability\r\n",
    "\r\n",
    "Understanding when to use a specific type of regression is key in statistical analysis. This table compares various regression types, their equations, best-suited use cases, and situations where they might not be the best choice.\r\n",
    "\r\n",
    "| Regression Type       | Equation                                             | Best Suited Use Cases | Not Best Suited For |\r\n",
    "|-----------------------|------------------------------------------------------|-----------------------|---------------------|\r\n",
    "| **Ordinary Least Squares (OLS)** | Y = β₀ + β₁X₁ + ... + βₙXₙ + ε                   | Basic linear regression tasks with data meeting linearity, independence, and homoscedasticity assumptions. | Data with multicollinearity, non-linear relationships, or significant outliers. |\r\n",
    "| **Ridge Regression (L2 Regularization)** | Y = β₀ + β₁X₁ + ... + βₙXₙ + λΣβᵢ² + ε           | Multicollinear data where independent variables are highly correlated. | Scenarios requiring feature selection; it doesn't reduce coefficients to zero. |\r\n",
    "| **Lasso Regression (L1 Regularization)** | Y = β₀ + β₁X₁ + ... + βₙXₙ + λΣ|βᵢ| + ε          | Situations with multicollinearity and necessity for feature selection (some coefficients can be zero). | Cases with fewer observations than features, or when features are highly correlated. |\r\n",
    "| **Elastic Net Regression** | Y = β₀ + β₁X₁ + ... + βₙXₙ + λ₁Σ|βᵢ| + λ₂Σβᵢ² + ε | Balancing feature selection of Lasso and regularization of Ridge, especially in complex datasets. | Simple linear problems where simpler models could be sufficient. |\r\n",
    "| **Quantile Regression** | Qᵧ(τ) = β₀(τ) + β₁(τ)X₁ + ... + βₙ(τ)Xₙ               | Datasets with non-constant variance, outliers, or when predicting different quantiles. | Data well approximated by the mean relationship between variables. |\r\n",
    "| **Logistic Regression**   | log(𝑝/(1-𝑝)) = β₀ + β₁X₁ + ... + βₙXₙ                | Binary outcome predictions like spam detection or credit default. | Continuous data prediction or multi-class classification without modifications. |\r\n",
    "| **Polynomial Regression** | Y = β₀ + β₁X + β₂X² + ... + βₙXⁿ + ε                 | Non-linear data relationships, such as growth rates or curved trends. | Linear data or when relationships are not well-captured by polynomials. |\r\n",
    "| **Multivariate Regression** | Multiple Y equations                               | Predicting multiple outcomes from a set of predictors. | Simple scenarios with a single outcome variable. |\r\n",
    "| **Stepwise Regression**   | Iterative process, starts with OLS and adds/removes variables. | Selecting the optimal subset of predictors from a large set. | High-dimensional datasets at risk of overfitting. |\r\n",
    "| **Robust Regression**     | Similar to OLS but adjusted for outliers.           | Data with outliers or influential observations not adhering to OLS assumptions. | Data perfectly meeting all OLS assumptions with no significant outliers. |\r\n",
    "\r\n",
    "Each regression method is uniquely suited to certain scenarios. Choosing the right type depends on the nature of the data and the specific requirements of the analysis.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d2b04-5347-4e1f-9f67-445e91286ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
