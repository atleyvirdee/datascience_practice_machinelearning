{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e938b2e6-f873-4ee5-ab0f-66a0b3b5dff3",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "## 1. Definition\n",
    "\n",
    "Policy Gradient Methods are a type of reinforcement learning techniques used in machine learning. Unlike other methods that learn a value function to make decisions, policy gradient methods directly learn the policy function that maps a given state to a probability distribution over actions. This approach adjusts the policy parameters in a direction that maximizes the expected reward. One of the key benefits of policy gradient methods is their ability to handle high-dimensional action spaces and continuous action spaces.\n",
    "\n",
    "## Explanation in Layman's Terms\n",
    "\n",
    "Imagine you're a chef experimenting with new recipes. Each ingredient, cooking time, and technique you choose is an 'action', and your goal is to find the perfect combination that results in a delicious dish (the 'reward'). In this scenario, you're not just randomly choosing actions; you're trying to develop an intuition or policy about what works best.\n",
    "\n",
    "Policy Gradient Methods are like gradually refining your cooking intuition through trial and error. Each time you prepare a dish, you take note of how well it was received (the reward). Then, you adjust your cooking approach (policy) slightly, based on what you've learned. For example, if adding a bit more salt and cooking at a lower temperature yielded a tastier dish, you'll be more inclined to try similar actions in the future.\n",
    "\n",
    "The key here is that you're not just learning rules like \"always cook at low heat\" or \"always add two pinches of salt.\" Instead, you're developing a nuanced strategy that considers the current state of the dish and then makes probabilistic decisions. Itâ€™s a flexible approach, allowing you to adapt and try new techniques while also leaning on what has worked well in the past.\n",
    "\n",
    "Over time, through many iterations of cooking and adjusting, you develop a sophisticated policy (or cooking style) that reliably produces tasty dishes, even in the face of new or complex recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07ab1c-09c8-40a0-a311-4b7484f78a5b",
   "metadata": {},
   "source": [
    "## 2. History of Policy Gradient Methods\n",
    "\n",
    "1. **Development and History**:\n",
    "\n",
    "- **Origins**: Policy Gradient Methods have roots in the reinforcement learning and neural network research of the 1980s and 1990s. These methods were formalized and gained prominence through the work of researchers such as Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour in the late 1990s and early 2000s.\n",
    "- **Purpose**: Policy Gradient Methods were developed as a class of algorithms in reinforcement learning that directly optimize the policy that an agent uses to decide on actions. Unlike value-based methods, which first learn the value of taking actions in various states, policy gradient methods learn a parameterized policy that can select actions without consulting a value function.\n",
    "\n",
    "2. **Name Origin**:\n",
    "\n",
    "- **Policy**: Refers to the strategy that the agent follows when deciding on actions to take in different situations. In reinforcement learning, a policy maps states of the environment to actions to be taken when in those states.\n",
    "- **Gradient**: Indicates the use of gradient ascent (or descent, depending on the formulation) for optimization. Policy Gradient Methods optimize the policy by adjusting the parameters in the direction of the gradient of some performance measure with respect to the policy parameters. This is done to maximize the expected return.\n",
    "\n",
    "The term \"Policy Gradient Methods\" succinctly captures the essence of these algorithms: they optimize the policy directly by following the gradient of expected returns. This approach allows for learning policies that can be more flexible and better suited to complex environments than those derived from value-based methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe140c-7df3-4c26-a5c9-89849a549855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
