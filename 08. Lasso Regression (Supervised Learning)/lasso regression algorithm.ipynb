{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a174fa2-9273-4ff7-b893-8e80aec4c7af",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "## Definition\n",
    "\n",
    "Lasso Regression, which stands for Least Absolute Shrinkage and Selection Operator, is a type of linear regression that uses shrinkage. It is used over regression methods for a more accurate prediction. This model uses shrinkage.  The lasso procedure encourages simple, sparse models (i.e., models with fewer parameters). This is achieved by imposing a penalty on the absolute size of the coefficients. The penalty applied is a lambda term multiplied by the sum of the absolute values of the coefficients.\n",
    "\n",
    "The formula for Lasso Regression is:\n",
    "\n",
    "Residual Sum of Squares + λ * (Sum of the absolute value of the magnitude of coefficients)\n",
    "\n",
    "In this formula, λ is a tuning parameter that determines the strength of the penalty; as λ increases, more coefficients are driven to zero and eliminated. This feature of Lasso Regression can be particularly useful for feature selection in models with a large number of predictors.\n",
    "\n",
    "## Explanation in Layman's Terms\n",
    "\n",
    "Imagine you are preparing a meal with a huge variety of ingredients on the table. However, your kitchen is small, and you can't use all of them. Lasso Regression is like a chef who picks the most important ingredients that will make the biggest difference in the taste of the dish. It disregards the less important ingredients, or in statistical terms, sets their coefficients to zero.\n",
    "\n",
    "This is particularly useful when you have a lot of potential predictors or features for a model, but you suspect that only a few of them really matter. Lasso Regression not only helps to improve the prediction but also makes the model simpler and easier to interpret by eliminating unnecessary features. It's like a smart chef who knows that sometimes less is more, and by choosing the right ingredients, you can make a dish (or a model) that is both simple and delicious (or effective).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aec4a2-9226-493e-be7d-0c0914222b96",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=NGf0voTMlcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad49e8de-e2de-4434-a131-599e96502d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(n_samples) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Fit Ridge, Lasso, and Linear Regression\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m ridge \u001b[38;5;241m=\u001b[39m \u001b[43mRidge\u001b[49m(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     11\u001b[0m ridge\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     13\u001b[0m lasso \u001b[38;5;241m=\u001b[39m Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Ridge' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "# Create some data points\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 50, 1\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = 3 * X.ravel() + np.random.randn(n_samples) * 2\n",
    "\n",
    "# Fit Ridge, Lasso, and Linear Regression\n",
    "ridge = Ridge(alpha=10)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X, y)\n",
    "\n",
    "# Generate points for prediction line\n",
    "x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "ridge_line = ridge.predict(x_plot[:, None])\n",
    "lasso_line = lasso.predict(x_plot[:, None])\n",
    "linear_line = linear.predict(x_plot[:, None])\n",
    "\n",
    "# Plotting the data points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='navy', s=30, marker='o', label=\"Training points\")\n",
    "\n",
    "# Plotting the prediction lines\n",
    "plt.plot(x_plot, ridge_line, color='teal', linewidth=2, label='Ridge regression')\n",
    "plt.plot(x_plot, lasso_line, color='magenta', linewidth=2, label='Lasso regression')\n",
    "plt.plot(x_plot, linear_line, color='orange', linewidth=2, label='Linear regression')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Feature value (X)')\n",
    "plt.ylabel('Target value (Y)')\n",
    "plt.title('Lasso vs. Ridge vs. Linear Regression')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Return the coefficients for interpretation\n",
    "(ridge.coef_[0], lasso.coef_[0], linear.coef_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19eb82-93ea-4c3a-8910-4502cebab839",
   "metadata": {},
   "source": [
    "The diagram depicts three regression models fitted to the same training data:\n",
    "\n",
    "Linear Regression (in orange) aims to minimize the residual sum of squares between the observed targets in the dataset and the targets predicted by the linear approximation.\n",
    "\n",
    "Ridge Regression (in teal) introduces L2 regularization, adding a penalty equal to the square of the magnitude of coefficients. This method is good for handling multicollinearity or when the number of predictors (features) exceeds the number of observations.\n",
    "\n",
    "Lasso Regression (in magenta) introduces L1 regularization, adding a penalty equal to the absolute value of the magnitude of coefficients. This results in sparse models where some coefficients can become zero and can be used for feature selection.\n",
    "\n",
    "Comparing the slopes (coefficients) of the fitted lines:\n",
    "\n",
    "The Ridge regression has a coefficient of \n",
    "2.512\n",
    "2.512.\n",
    "The Lasso regression has a coefficient of \n",
    "2.830\n",
    "2.830.\n",
    "The Linear regression has a coefficient of \n",
    "2.909\n",
    "2.909.\n",
    "The differences in these coefficients illustrate the effect of regularization. Lasso tends to push coefficients toward zero, potentially setting some of them exactly to zero, hence producing a simpler model that could be better at generalizing when there's unnecessary complexity in the model. Ridge adjusts the coefficients to be smaller but rarely makes them zero.\n",
    "\n",
    "The choice between these models typically depends on the problem at hand, the presence of multicollinearity, the goal of feature selection, and the need to prevent overfitting. If the goal is purely prediction accuracy, cross-validation can be used to assess which model performs best on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798df23e-adb9-4547-a1c8-b28861a7537c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
