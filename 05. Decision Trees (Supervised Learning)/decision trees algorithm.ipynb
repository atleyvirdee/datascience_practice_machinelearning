{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4edebe-83c5-4715-a0ce-70b31cb7157c",
   "metadata": {},
   "source": [
    "# Decision Trees\r\n",
    "\r\n",
    "## Definition\r\n",
    "\r\n",
    "Decision Trees are a type of supervised learning algorithm that can be used for both classification and regression tasks, but are more commonly used for classification. They work by breaking down a dataset into smaller and smaller subsets based on different criteria, while at the same time an associated tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches, each representing values for the attribute tested. Leaf node represents a decision on the numerical target. The topmost decision node in a tree corresponds to the best predictor called root node. Decision trees handle both categorical and numerical data.\r\n",
    "\r\n",
    "The paths from root to leaf represent classification rules. One of the biggest advantages of decision trees is their interpretability â€“ they are very easy to understand and interpret, and their decisions can be visualized.\r\n",
    "\r\n",
    "## Explanation in Layman's Terms\r\n",
    "\r\n",
    "Imagine you're trying to decide where to go on vacation. A decision tree is like a flowchart that helps you make this decision. It starts with a big question (like \"Do I want adventure or relaxation?\") and based on your answer, it takes you down a path to the next question, and so on. Each step of the way, you're narrowing down your options.\r\n",
    "\r\n",
    "For example, if you choose adventure, the next question might be \"Do I prefer hiking or surfing?\" Your choices continue to guide you down this tree until you reach a final decision (like choosing a mountain trip for hiking). \r\n",
    "\r\n",
    "In real-life data scenarios, a decision tree does something similar with data. It looks at the different characteristics (or 'features') of the data and makes a series of decisions to categorize or 'classify' the data into different groups. It's like having a roadmap that guides you through the data to help you understand how different decisions lead to different outcomes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3163b62c-3549-45b2-b858-ec890b5d23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `twice` not found.\n"
     ]
    }
   ],
   "source": [
    "descioin tree on same data run twice??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884a99f-79ee-4edf-8da7-924719b768f9",
   "metadata": {},
   "source": [
    "- **History**: The modern decision tree algorithms were developed in the early 1960s by Anthony Morgan and others.\n",
    "- **Meaning**: Named for their tree-like branching structure where each node represents a decision.\n",
    "## History of Decision Trees\n",
    "\n",
    "Decision Trees are a popular machine learning algorithm used for both classification and regression tasks. The development of decision trees was driven by the need for intuitive and interpretable models that can make decisions based on a series of rules.\n",
    "\n",
    "### Origin and Early Development\n",
    "- **Inception in the 1960s**: The concept of decision trees can be traced back to the work on decision theory and sequential decision processes in the 1960s.\n",
    "- **Pioneered by Multiple Researchers**: Early versions of decision trees were developed independently by various researchers, including J. Ross Quinlan, who developed the ID3 (Iterative Dichotomiser 3) algorithm.\n",
    "\n",
    "### The Statistical and Computational Breakthrough\n",
    "- **ID3 Algorithm**: Developed by J. Ross Quinlan in the 1980s, ID3 is one of the first algorithms used to create a decision tree and laid the foundation for subsequent algorithms like C4.5 and CART (Classification and Regression Trees).\n",
    "- **CART Algorithm**: Introduced by Breiman, Friedman, Olshen, and Stone in 1984, the CART algorithm introduced the concept of binary recursive partitioning, allowing for more complex trees.\n",
    "\n",
    "### Why Were Decision Trees Developed?\n",
    "- **Need for Non-Linear Decision Making**: Unlike linear models, decision trees can capture non-linear relationships between features and the target variable.\n",
    "- **Interpretability and Simplicity**: Decision trees are easy to interpret and visualize, making them a go-to algorithm for understanding the decision-making process.\n",
    "- **Versatility**: Their ability to handle both categorical and continuous data made them suitable for a variety of applications.\n",
    "\n",
    "### Modern Usage and Significance\n",
    "- **Wide Range of Applications**: Today, decision trees are used in numerous fields, from medical diagnosis to financial analysis.\n",
    "- **Foundation for Ensemble Methods**: Decision trees are the building blocks of ensemble methods like Random Forests and Gradient Boosting Machines.\n",
    "\n",
    "### Conclusion\n",
    "The development of decision trees marked a significant milestone in the evolution of machine learning algorithms. Their intuitive structure, coupled with their ability to handle complex decision-making processes, has made them a staple in the field of data science and analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
