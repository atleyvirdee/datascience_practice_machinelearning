{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc43c376-e3b8-4272-90bd-65f9b897d954",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "## 1. Overview\n",
    "\n",
    "Naive Bayes is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, a fundamental theorem in probability theory. The 'naive' aspect of the name comes from the assumption that the features (or predictors) that go into the model are independent of each other. This is a simplifying assumption that, while not always true in real-world data, allows the algorithm to be efficient and perform well, especially in the case of text classification and spam filtering.\n",
    "\n",
    "Bayes' Theorem, in this context, is used to calculate the probability of a hypothesis (like whether an email is spam or not spam) based on prior knowledge of conditions that might be related to the hypothesis (like the presence of certain words in the email).\n",
    "\n",
    "## Explanation in Layman's Terms\n",
    "\n",
    "Let's say you have a basket of fruits which are either apples or oranges, and you want to determine the likelihood of picking an apple based on some of its features like color, size, and shape. Naive Bayes helps in making this determination.\n",
    "\n",
    "Imagine that you know some general facts like apples are generally red, and oranges are mostly orange in color. If you pick a fruit randomly and see it's red, Naive Bayes uses the color information to increase the likelihood in your mind that the fruit is an apple. It does this by calculating probabilities based on the features (color in this case) and what you already know about apples and oranges.\n",
    "\n",
    "The reason it's called 'naive' is because it assumes that each feature (like color, size, shape) contributes independently to the fruit being an apple or an orange. This is like assuming that the color of the fruit doesn’t affect its size or shape, which simplifies the calculation but isn’t always true in real life.\n",
    "\n",
    "Despite this simplification, Naive Bayes can be surprisingly effective and is particularly popular in tasks like email spam detection, where it looks at words in the emails and decides if an email is spam or not based on what it has learned from previous examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19392b-219b-4d98-9e2f-8374a01cd4f4",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273aef37-ba47-411a-aaad-d7185588136e",
   "metadata": {},
   "source": [
    "## 2. History of Naive Bayes\n",
    "\n",
    "* **Based on Bayes' Theorem**:  The Naive Bayes classifier is based on Bayes' Theorem, developed by Thomas Bayes in the 18th century, but the \"naive\" version as used today was developed in the mid-20th century for document classification.\n",
    "* **Naive**: It's termed \"naive\" because it assumes independence among predictor variables within the model, a simplification often considered unrealistic in real-world scenarios. This independence assumption simplifies the computation, making Naive Bayes a fast and effective model for certain types of data, despite its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e698534-f8d7-49c4-a932-1ad656709590",
   "metadata": {},
   "source": [
    "## 3. Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aabd26d-3637-4d3d-85aa-16d9cda9a1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "Prediction for new data: Approved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neurabytes-taranjit\\.virtualenvs\\datascience_practice_machinelearning-Sjf4ONJ-\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample dataset creation\n",
    "data = {\n",
    "    \"Credit_Score\": [750, 680, 720, 610, 590, 770, 650, 740, 580, 600],\n",
    "    \"Income\": [75000, 45000, 55000, 32000, 27000, 90000, 35000, 62000, 25000, 28000],\n",
    "    \"Loan_Amount\": [200000, 100000, 150000, 80000, 70000, 220000, 90000, 160000, 60000, 85000],\n",
    "    \"Loan_Approved\": [1, 0, 1, 0, 0, 1, 0, 1, 0, 0]  # 1 = Approved, 0 = Rejected\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[[\"Credit_Score\", \"Income\", \"Loan_Amount\"]]\n",
    "y = df[\"Loan_Approved\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting a new data point\n",
    "new_data = [[700, 50000, 120000]]  # Credit_Score, Income, Loan_Amount\n",
    "prediction = nb_model.predict(new_data)\n",
    "print(\"Prediction for new data:\", \"Approved\" if prediction[0] == 1 else \"Rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af8404-ad33-41f6-8588-0a836b674f94",
   "metadata": {},
   "source": [
    "## 4. Usecases in Finance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f515d0-d3fe-491c-af77-3b887585c861",
   "metadata": {},
   "source": [
    "- **Fraud Detection:** Classifying transactions as fraudulent or legitimate based on transaction metadata and historical patterns.\n",
    "\n",
    "- **Credit Scoring:** Categorizing loan applicants into risk categories (low, medium, high) based on credit history and other financial attributes.\n",
    "\n",
    "- **Spam Detection in Financial Emails:** Identifying phishing or spam emails targeting financial clients using email text features.\n",
    "\n",
    "- **Customer Segmentation:** Grouping customers into predefined categories based on spending habits, income levels, and product usage patterns.\n",
    "\n",
    "- **Predicting Customer Attrition:** Estimating the likelihood of customers leaving a financial service or product.\n",
    "\n",
    "- **Loan Default Prediction:** Classifying borrowers based on their likelihood of default using historical data and financial indicators.\n",
    "\n",
    "- **Sentiment Analysis for Market Predictions:** Analyzing news headlines or social media sentiment to classify the market as bullish, bearish, or neutral.\n",
    "\n",
    "- **Insurance Claim Classification:** Categorizing insurance claims into fraudulent or genuine based on historical claims data and customer profiles.\n",
    "\n",
    "- **Portfolio Risk Assessment:** Classifying assets or portfolios into risk categories (low, medium, high) based on their historical performance.\n",
    "\n",
    "- **Marketing Campaign Classification:** Identifying the most likely responders to financial product campaigns using demographic and past interaction data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d6fc3-04f7-42a0-a9a2-caa03631a48e",
   "metadata": {},
   "source": [
    "## 5.  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083010-a7f9-440c-9c1c-d44b20fd31e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0089fcce-09a1-4217-a198-7ed9258a9260",
   "metadata": {},
   "source": [
    "# Comparing Logistic Regression and Naive Bayes\r\n",
    "\r\n",
    "## Fundamental Approach\r\n",
    "\r\n",
    "- **Logistic Regression:** \r\n",
    "  - A predictive analysis algorithm based on the concept of probability.\r\n",
    "  - Uses a logistic function to model a binary dependent variable.\r\n",
    "  - Estimates the probability of a binary outcome based on one or more independent variables.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - A classification technique based on Bayes' Theorem with an assumption of independence among predictors.\r\n",
    "  - Particularly known for text classification problems where it considers conditional probability of each word/class.\r\n",
    "\r\n",
    "## Assumptions\r\n",
    "\r\n",
    "- **Logistic Regression:**\r\n",
    "  - Assumes a linear relationship between the log-odds of the dependent variable and the independent variables.\r\n",
    "  - Requires the independent variables to be linearly related to the log odds.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Assumes that all features (predictors) are independent of each other, which is the 'naive' part.\r\n",
    "  - Works well in cases where this assumption holds true, especially in high-dimensional datasets.\r\n",
    "\r\n",
    "## Data Suitability\r\n",
    "\r\n",
    "- **Logistic Regression:**\r\n",
    "  - Better suited for cases where there is a direct relationship between the independent and dependent variables.\r\n",
    "  - Often used in binary classification problems like spam detection, credit scoring, disease diagnosis.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Highly efficient with large datasets, particularly in text classification (like spam filtering, sentiment analysis).\r\n",
    "  - Performs well in multi-class prediction problems.\r\n",
    "\r\n",
    "## Performance\r\n",
    "\r\n",
    "- **Logistic Regression:**\r\n",
    "  - Can provide probabilities for outcomes and is robust to a noisy dataset.\r\n",
    "  - Requires careful feature selection to avoid overfitting and underfitting.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Generally faster and can be more efficient with a large number of features.\r\n",
    "  - Performs well even with less training data if the assumption of independence holds.\r\n",
    "\r\n",
    "## Use Cases\r\n",
    "\r\n",
    "- **Logistic Regression:** \r\n",
    "  - Ideal for problems where you have a dataset with numeric and categorical variables and you want to predict a binary outcome.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Excellent for scenarios with large feature spaces as in text classification, where the independence assumption simplifies the computation significantly.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875c258-8ab5-4bc5-b4c2-b69cdb127663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
