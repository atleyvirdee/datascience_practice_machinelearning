{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d687e23-cc4e-48fe-941d-df1d2f22dd0b",
   "metadata": {},
   "source": [
    "# Naive Bayes\r\n",
    "## 1. Overviewon\r\n",
    "\r\n",
    "Naive Bayes is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, a fundamental theorem in probability theory. The 'naive' aspect of the name comes from the assumption that the features (or predictors) that go into the model are independent of each other. This is a simplifying assumption that, while not always true in real-world data, allows the algorithm to be efficient and perform well, especially in the case of text classification and spam filtering.\r\n",
    "\r\n",
    "Bayes' Theorem, in this context, is used to calculate the probability of a hypothesis (like whether an email is spam or not spam) based on prior knowledge of conditions that might be related to the hypothesis (like the presence of certain words in the email).\r\n",
    "\r\n",
    "## Explanation in Layman's Terms\r\n",
    "\r\n",
    "Let's say you have a basket of fruits which are either apples or oranges, and you want to determine the likelihood of picking an apple based on some of its features like color, size, and shape. Naive Bayes helps in making this determination.\r\n",
    "\r\n",
    "Imagine that you know some general facts like apples are generally red, and oranges are mostly orange in color. If you pick a fruit randomly and see it's red, Naive Bayes uses the color information to increase the likelihood in your mind that the fruit is an apple. It does this by calculating probabilities based on the features (color in this case) and what you already know about apples and oranges.\r\n",
    "\r\n",
    "The reason it's called 'naive' is because it assumes that each feature (like color, size, shape) contributes independently to the fruit being an apple or an orange. This is like assuming that the color of the fruit doesn’t affect its size or shape, which simplifies the calculation but isn’t always true in real life.\r\n",
    "\r\n",
    "Despite this simplification, Naive Bayes can be surprisingly effective and is particularly popular in tasks like email spam detection, where it looks at words in the emails and decides if an email is spam or not based on what it has learned from previous examples.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19392b-219b-4d98-9e2f-8374a01cd4f4",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?app=desktop&v=O2L2Uv9pdDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089fcce-09a1-4217-a198-7ed9258a9260",
   "metadata": {},
   "source": [
    "# Comparing Logistic Regression and Naive Bayes\r\n",
    "\r\n",
    "## Fundamental Approach\r\n",
    "\r\n",
    "- **Logistic Regression:** \r\n",
    "  - A predictive analysis algorithm based on the concept of probability.\r\n",
    "  - Uses a logistic function to model a binary dependent variable.\r\n",
    "  - Estimates the probability of a binary outcome based on one or more independent variables.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - A classification technique based on Bayes' Theorem with an assumption of independence among predictors.\r\n",
    "  - Particularly known for text classification problems where it considers conditional probability of each word/class.\r\n",
    "\r\n",
    "## Assumptions\r\n",
    "\r\n",
    "- **Logistic Regression:**\r\n",
    "  - Assumes a linear relationship between the log-odds of the dependent variable and the independent variables.\r\n",
    "  - Requires the independent variables to be linearly related to the log odds.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Assumes that all features (predictors) are independent of each other, which is the 'naive' part.\r\n",
    "  - Works well in cases where this assumption holds true, especially in high-dimensional datasets.\r\n",
    "\r\n",
    "## Data Suitability\r\n",
    "\r\n",
    "- **Logistic Regression:**\r\n",
    "  - Better suited for cases where there is a direct relationship between the independent and dependent variables.\r\n",
    "  - Often used in binary classification problems like spam detection, credit scoring, disease diagnosis.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Highly efficient with large datasets, particularly in text classification (like spam filtering, sentiment analysis).\r\n",
    "  - Performs well in multi-class prediction problems.\r\n",
    "\r\n",
    "## Performance\r\n",
    "\r\n",
    "- **Logistic Regression:**\r\n",
    "  - Can provide probabilities for outcomes and is robust to a noisy dataset.\r\n",
    "  - Requires careful feature selection to avoid overfitting and underfitting.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Generally faster and can be more efficient with a large number of features.\r\n",
    "  - Performs well even with less training data if the assumption of independence holds.\r\n",
    "\r\n",
    "## Use Cases\r\n",
    "\r\n",
    "- **Logistic Regression:** \r\n",
    "  - Ideal for problems where you have a dataset with numeric and categorical variables and you want to predict a binary outcome.\r\n",
    "\r\n",
    "- **Naive Bayes:**\r\n",
    "  - Excellent for scenarios with large feature spaces as in text classification, where the independence assumption simplifies the computation significantly.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875c258-8ab5-4bc5-b4c2-b69cdb127663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
