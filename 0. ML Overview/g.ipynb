{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab58a3c0-eb67-4df8-8fdd-7e513b70c3d8",
   "metadata": {},
   "source": [
    "Learning Rate\n",
    "Loss Function\n",
    "Bagging\n",
    "Boosting\n",
    "Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978c90a-6aeb-422f-9a79-e470d9094876",
   "metadata": {},
   "source": [
    "* basic\n",
    "* laymen\n",
    "* list of similar algos\n",
    "* history of algo\n",
    "* intuition of algo\n",
    "* comparison to other algos\n",
    "* sample code\n",
    "* hyperparameters of this algo\n",
    "* evaluation of this algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8da408-ffb0-449c-8d62-a9b45891f4fc",
   "metadata": {},
   "source": [
    "| Name of Algorithm | Description | Type of Algo | Year |\n",
    "|-------------------|-------------|--------------|------|\n",
    "| **Linear Regression** | Predicts a continuous output variable based on one or more input features. | Supervised Learning | Late 19th century |\n",
    "| **Principal Component Analysis (PCA)** | A dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information. | Unsupervised Learning | 1930s |\n",
    "| **Logistic Regression** | Used for binary classification tasks (predicting a binary outcome). | Supervised Learning | 1958 |\n",
    "| **k-Nearest Neighbors (kNN)** | Classifies a data point based on how its neighbors are classified. | Supervised Learning | 1952 |\n",
    "| **Naive Bayes** | A group of algorithms based on applying Bayes' theorem with strong independence assumptions between features. | Supervised Learning | Mid 20th century |\n",
    "| **Ridge Regression** | Addresses some of the problems of ordinary least squares by imposing a penalty on the size of coefficients. | Supervised Learning | 1960s |\n",
    "| **K-Means Clustering** | Partitions n observations into k clusters where each observation belongs to the cluster with the nearest mean. | Unsupervised Learning | 1967 |\n",
    "| **Decision Trees** | A flowchart-like tree structure where an internal node represents a feature, and each leaf node represents a decision outcome. | Supervised Learning | 1980s |\n",
    "| **Self-Organizing Maps (SOMs)** | An unsupervised learning algorithm that reduces the dimensions of data through a neural network. | Unsupervised Learning | 1980s |\n",
    "| **Temporal Difference (TD) Learning** | A mix of Monte Carlo ideas and dynamic programming methods. | Reinforcement Learning | 1980s |\n",
    "| **Support Vector Machines (SVM)** | Finds a hyperplane in an N-dimensional space that distinctly classifies data points. | Supervised Learning | 1995 |\n",
    "| **AdaBoost (Adaptive Boosting)** | Combines multiple weak classifiers to increase the accuracy of classifiers. | Supervised Learning | 1996 |\n",
    "| **DBSCAN** | A density-based clustering algorithm. | Unsupervised Learning | 1996 |\n",
    "| **Lasso Regression** | Performs L1 regularization to allow for feature selection. | Supervised Learning | 1996 |\n",
    "| **Independent Component Analysis (ICA)** | A computational method to separate a multivariate signal into additive independent non-Gaussian signals. | Unsupervised Learning | 1994 |\n",
    "| **Apriori** | Identifies the frequent individual items in a dataset and extends them to larger and larger item sets as long as those item sets appear sufficiently often in the database. | Association Rule Learning | 1994 |\n",
    "| **Support Vector Regression (SVR)** | A type of SVM used for regression challenges. | Supervised Learning | Late 1990s |\n",
    "| **Gradient Boosting Machines (GBM)** | An ensemble technique that builds models sequentially, each correcting its predecessor. | Supervised Learning | Late 1990s |\n",
    "| **ECLAT** | Improves the Apriori algorithm by using a vertical database layout and a depth-first search. | Association Rule Learning | 1997 |\n",
    "| **Long Short-Term Memory Networks (LSTM)** | A type of RNN capable of learning order dependence in sequence prediction problems. | Deep Learning | 1997 |\n",
    "| **FP Growth** | Efficiently mines the complete set of frequent itemsets without candidate generation. | Association Rule Learning | 2000 |\n",
    "| **Random Forest** | An ensemble of decision trees, typically used for classification problems. | Supervised Learning | 2001 |\n",
    "| **Elastic Net Regression** | A regularized regression method that linearly combines L1 and L2 penalties of the Lasso and Ridge methods. | Supervised Learning | 2005 |\n",
    "| **MCTS** | Uses random sampling and a tree structure to make decisions in a large decision space, notably in games. | Decision Making | 2006 |\n",
    "| **Affinity Propagation** | Creates clusters by sending messages between pairs of samples. | Unsupervised Learning | 2007 |\n",
    "| **t-Distributed Stochastic Neighbor Embedding (t-SNE)** | A non-linear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization. | Unsupervised Learning | 2008 |\n",
    "| **Extreme Gradient Boosting (XGBoost)** | A scalable and accurate implementation of gradient boosting machines. | Supervised Learning | 2014 |\n",
    "| **GRU** | Simplifies the LSTM architecture with fewer parameters while maintaining similar performance. | Deep Learning | 2014 |\n",
    "| **Deep Q-Network (DQN)** | Combines Q-learning with deep neural networks. | Reinforcement Learning | 2013 |\n",
    "| **Generative Adversarial Networks (GANs)** | Consists of two networks, a generator and a discriminator, which contest with each other. | Deep Learning | 2014 |\n",
    "| **Trust Region Policy Optimization (TRPO)** | Maximizes a surrogate objective function using trust region methods. | Reinforcement Learning | 2015 |\n",
    "| **U-Net** | Used for fast and precise segmentation of images. | Deep Learning | 2015 |\n",
    "| **YOLO (You Only Look Once)** | A real-time object detection system. | Deep Learning | 2016 |\n",
    "| **CatBoost** | An algorithm that uses gradient boosting on decision trees, with support for categorical variables. | Supervised Learning | 2017 |\n",
    "| **LightGBM** | A gradient boosting framework designed for speed and efficiency. | Supervised Learning | 2017 |\n",
    "| **Proximal Policy Optimization (PPO)** | A policy gradient method for reinforcement learning. | Reinforcement Learning | 2017 |\n",
    "| **Transformer Networks** | Introduces self-attention mechanisms for sequence-to-sequence tasks, revolutionizing NLP. | Deep Learning | 2017 |\n",
    "| **UMap** | A manifold learning technique for dimensionality reduction, focusing on preserving both local and global structure. | Dimensionality Reduction | 2018 |\n",
    "| **BERT (Bidirectional Encoder Representations from Transformers)** | Designed to understand the context of a word in a sentence, bidirectionally. | Deep Learning | 2018 |\n",
    "| **GPT** | A pre-trained transformer model that generates human-like text based on a given prompt. | Deep Learning | 2018 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a48aea-5ab1-415f-a5f6-4ac55ede1be7",
   "metadata": {},
   "source": [
    "| Name of Algorithm | Description | Type - of Algo | Year |\n",
    "|-------------------|-------------|----------------|------|\n",
    "| **Apriori** | Identifies the frequent individual items in a dataset and extends them to larger and larger item sets as long as those item sets appear sufficiently often in the database. | - Association Rule Learning <br> | 1994 |\n",
    "| **ECLAT** | Equivalence Class Clustering and bottom-up Lattice Traversal for frequent item set mining, a depth-first search algorithm. | - Association Rule Learning <br> | 1997 |\n",
    "| **FP Growth** | Frequent Pattern Growth, for mining frequent itemsets for boolean association rules. | - Association Rule Learning <br> | 2000 |\n",
    "| **Temporal Difference (TD) Learning** | A mix of Monte Carlo ideas and dynamic programming methods. | - Reinforcement Learning <br> | 1980s |\n",
    "| **Q-Learning** | A model-free reinforcement learning algorithm to learn the value of an action in a particular state. | - Reinforcement Learning <br> | 1992 |\n",
    "| **Deep Q-Network (DQN)** | Combines Q-learning with deep neural networks. | - Reinforcement Learning <br> | 2013 |\n",
    "| **Proximal Policy Optimization (PPO)** | A policy gradient method for reinforcement learning. | - Reinforcement Learning <br> | 2017 |\n",
    "| **Trust Region Policy Optimization (TRPO)** | Maximizes a surrogate objective function using trust region methods. | - Reinforcement Learning <br> | 2015 |\n",
    "| **Policy Gradient Methods** | Methods for optimizing policies with respect to long-term returns by gradient descent. | - Reinforcement Learning <br> | 1990s |\n",
    "| **Actor Critic** | Combines policy gradient and value function methods for optimizing policies. | - Reinforcement Learning <br> | early 21st century |\n",
    "| **MCTS** | Monte Carlo Tree Search, a heuristic search algorithm for decision processes. | - Reinforcement Learning <br> | 2006 |\n",
    "| **UMap** | Uniform Manifold Approximation and Projection, for dimension reduction based on manifold learning. | - Unsupervised Learning <br> | 2018 |\n",
    "| **VAEs** | Variational Autoencoders, a generative model for learning latent representations. | - Deep Learning <br> | 2010s |\n",
    "| **GRU** | Gated Recurrent Unit, simplifies the LSTM model for sequence processing. | - Deep Learning <br> | 2014 |\n",
    "| **Transformer Networks** | Introduces a model architecture eschewing recurrence and convolutions entirely in favor of attention mechanisms. | - Deep Learning <br> | 2017 |\n",
    "| **GPT** | Generative Pre-trained Transformer, a model for natural language understanding and generation. | - Deep Learning <br> | 2018 |\n",
    "| **Independent Component Analysis (ICA)** | A computational method to separate a multivariate signal into additive independent non-Gaussian signals. | - Unsupervised Learning <br> | 1994 |\n",
    "| **t-Distributed Stochastic Neighbor Embedding (t-SNE)** | A non-linear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization. | - Unsupervised Learning <br> | 2008 |\n",
    "| **Spectral Clustering** | Uses the spectrum (eigenvalues) of the similarity matrix to reduce dimensions before clustering. | - Unsupervised Learning <br> | Late 20th century |\n",
    "| **Affinity Propagation** | Creates clusters by sending messages between pairs of samples. | - Unsupervised Learning <br> | 2007 |\n",
    "| **DBSCAN** | A density-based clustering algorithm. | - Unsupervised Learning <br> | 1996 |\n",
    "| **Principal Component Analysis (PCA)** | A dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information. | - Unsupervised Learning <br> | 1930s |\n",
    "| **Hierarchical Clustering** | Builds a hierarchy of clusters either through a bottom-up (agglomerative) or top-down (divisive) approach. | - Unsupervised Learning <br> | Early to mid-20th century |\n",
    "| **K-Means Clustering** | Partitions n observations into k clusters where each observation belongs to the cluster with the nearest mean. | - Unsupervised Learning <br> | 1967 |\n",
    "| **Self-Organizing Maps (SOMs)** | An unsupervised learning algorithm that reduces the dimensions of data through a neural network. | - Unsupervised Learning <br> | 1980s |\n",
    "| **CatBoost** | An algorithm that uses gradient boosting on decision trees, with support for categorical variables. | - Supervised Learning | 2017 |\n",
    "| **LightGBM** | A gradient boosting framework designed for speed and efficiency. | - Supervised Learning <br> | 2017 by Microsoft |\n",
    "| **Extreme Gradient Boosting (XGBoost)** | A scalable and accurate implementation of gradient boosting machines. | - Supervised Learning <br> | 2014 |\n",
    "| **Gradient Boosting Machines (GBM)** | An ensemble technique that builds models sequentially, each correcting its predecessor. | - Supervised Learning <br> | late 1990s |\n",
    "| **AdaBoost (Adaptive Boosting)** | Combines multiple weak classifiers to increase the accuracy of classifiers. | - Supervised Learning <br> | 1996 |\n",
    "| **Random Forest** | An ensemble of decision trees, typically used for classification problems. | - Supervised Learning <br> | 2001 |\n",
    "| **Support Vector Regression (SVR)** | A type of SVM used for regression challenges. | - Supervised Learning <br> | late 1990s |\n",
    "| **Elastic Net Regression** | A regularized regression method that linearly combines L1 and L2 penalties of the Lasso and Ridge methods. | - Supervised Learning <br> | 2005 |\n",
    "| **Lasso Regression** | Performs L1 regularization to allow for feature selection. | - Supervised Learning <br> | 1996 |\n",
    "| **Ridge Regression** | Addresses some of the problems of ordinary least squares by imposing a penalty on the size of coefficients. | - Supervised Learning <br> | 1960s |\n",
    "| **Support Vector Machines (SVM)** | Finds a hyperplane in an N-dimensional space that distinctly classifies data points. | - Supervised Learning <br> | 1995 |\n",
    "| **Decision Trees** | A flowchart-like tree structure where an internal node represents a feature, and each leaf node represents a decision outcome. | - Supervised Learning <br> | 1980s |\n",
    "| **Naive Bayes** | A group of algorithms based on applying Bayes' theorem with strong independence assumptions between features. | - Supervised Learning <br> | Mid 20th century |\n",
    "| **k-Nearest Neighbors (kNN)** | Classifies a data point based on how its neighbors are classified. | - Supervised Learning <br> | 1952 |\n",
    "| **Logistic Regression** | Used for binary classification tasks (predicting a binary outcome). | - Supervised Learning <br> | 1958 |\n",
    "| **Linear Regression** | Predicts a continuous output variable based on one or more input features. | - Supervised Learning <br> | Late 19th century |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd1be9-eaa5-4401-9f0c-c8e99c216c48",
   "metadata": {},
   "source": [
    "| Name of Algorithm | Description | Type - of Algo | Year |\n",
    "|---|---|---|---|\n",
    "| [**Linear Regression**](01.%20Linear%20Regression%20(Supervised%20Learning)/1.1%20Linear%20Regression.ipynb) | Predicts a continuous output variable based on one or more input features. | - Supervised Learning <br> | Late 19th century |\n",
    "| [**Principal Component Analysis (PCA)**](https://example.com) | A dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information. | - Unsupervised Learning <br> | 1930s |\n",
    "| [**Logistic Regression**](02.%20Logistic%20Regression%20(Supervised%20Learning)/2.1%20Logistic%20Regression.ipynb) | Used for binary classification tasks (predicting a binary outcome). | - Supervised Learning <br> | 1958 |\n",
    "| [**k-Nearest Neighbors (kNN)**](04.%20k-Nearest%20Neighbors%20(kNN)%20(Supervised%20Learning)/4.1%20K-Nearest%20Neighbors.ipynb) | Classifies a data point based on how its neighbors are classified. | - Supervised Learning <br> | 1952 |\n",
    "| [**Naive Bayes**](03.%20Naive%20Bayes%20(Supervised%20Learning)/3.1%20Naive%20bayes.ipynb) | A group of algorithms based on applying Bayes' theorem with strong independence assumptions between features. | - Supervised Learning <br> | Mid 20th century |\n",
    "| [**Ridge Regression**](07.%20Ridge%20Regression%20(Supervised%20Learning)/7.1%20Ridge%20Regression.ipynb) | Addresses some of the problems of ordinary least squares by imposing a penalty on the size of coefficients. | - Supervised Learning <br> | 1960s |\n",
    "| [**K-Means Clustering**](https://example.com) | Partitions n observations into k clusters where each observation belongs to the cluster with the nearest mean. | - Unsupervised Learning <br> | 1967 |\n",
    "| [**Decision Trees**](05.%20Decision%20Trees%20(Supervised%20Learning)/5.1%20Decision%20Trees.ipynb) | A flowchart-like tree structure where an internal node represents a feature, and each leaf node represents a decision outcome. | - Supervised Learning <br> | 1980s |\n",
    "| [**Self-Organizing Maps (SOMs)**](https://example.com) | An unsupervised learning algorithm that reduces the dimensions of data through a neural network. | - Unsupervised Learning <br> | 1980s |\n",
    "| [**Temporal Difference (TD) Learning**](https://example.com) | A mix of Monte Carlo ideas and dynamic programming methods. | - Reinforcement Learning <br> | 1980s |\n",
    "| [**Artificial Neural Networks (ANN)**](https://example.com) | Consists of 'neurons' arranged in layers that process data based on a set of weights and activation functions. | - Deep Learning <br> | 1980s |\n",
    "| [**Convolutional Neural Networks (CNN)**](https://example.com) | Particularly effective for image recognition and processing tasks. | - Deep Learning <br> | 1980s |\n",
    "| [**Recurrent Neural Networks (RNN)**](https://example.com) | Suitable for processing sequences of data by having loops to allow information persistence. | - Deep Learning <br> | 1980s |\n",
    "| [**Autoencoders**](https://example.com) | Neural networks used for unsupervised learning of efficient codings. | - Deep Learning <br> | 1980s |\n",
    "| [**Policy Gradient Methods**](https://example.com) |  |  | 1990s |\n",
    "| [**Support Vector Machines (SVM)**](06.%20Support%20Vector%20Machines%20(SVM)%20(Supervised%20Learning)/6.1%20Support%20Vector%20Machines.ipynb) | Finds a hyperplane in an N-dimensional space that distinctly classifies data points. | - Supervised Learning <br> | 1995 |\n",
    "| [**AdaBoost (Adaptive Boosting)**](12.%20AdaBoost%20(Supervised%20Learning,%20Ensemble%20Method)/12.1%20Adaboost.ipynb) | Combines multiple weak classifiers to increase the accuracy of classifiers. | - Supervised Learning <br> | 1996 |\n",
    "| [**DBSCAN**](https://example.com) | A density-based clustering algorithm. | - Unsupervised Learning <br> | 1996 |\n",
    "| [**Lasso Regression**](08.%20Lasso%20Regression%20(Supervised%20Learning)/8.1%20Lasso%20Regression.ipynb) | Performs L1 regularization to allow for feature selection. | - Supervised Learning <br> | 1996 |\n",
    "| [**Independent Component Analysis (ICA)**](https://example.com) | A computational method to separate a multivariate signal into additive independent non-Gaussian signals. | - Unsupervised Learning <br> | 1994 |\n",
    "| [**Apriori**](https://example.com) |  |  | 1994 |\n",
    "| [**Support Vector Regression (SVR)**](10.%20Support%20Vector%20Regression%20(SVR)%20(Supervised%20Learning)/10.1%20Support%20Vector%20Regression.ipynb) | A type of SVM used for regression challenges. | - Supervised Learning <br> | late 1990s |\n",
    "| [**Gradient Boosting Machines (GBM)**](13.%20Gradient%20Boosting%20Machines%20(GBM)%20(Supervised%20Learning,%20Ensemble%20Method)/13.1%20Gradient%20Boosting%20Machines.ipynb) | An ensemble technique that builds models sequentially, each correcting its predecessor. | - Supervised Learning <br> | late 1990s |\n",
    "| [**ECLAT**](https://example.com) |  |  | 1997 |\n",
    "| [**Long Short-Term Memory Networks (LSTM)**](https://example.com) | A type of RNN capable of learning order dependence in sequence prediction problems. | - Deep Learning <br> | 1997 |\n",
    "| [**Random Forest**](11.%20Random%20Forest%20(Supervised%20Learning,%20Ensemble%20Method)/11.1%20Random%20Forest.ipynb) | An ensemble of decision trees, typically used for classification problems. | - Supervised Learning <br> | 2001 |\n",
    "| [**FP Growth**](https://example.com) |  |  | 2000 |\n",
    "| [**Elastic Net Regression**](09.%20Elastic%20Net%20Regression%20(Supervised%20Learning)/9.1%20Elastic%20Net.ipynb) | A regularized regression method that linearly combines L1 and L2 penalties of the Lasso and Ridge methods. | - Supervised Learning <br> | 2005 |\n",
    "| [**MCTS**](https://example.com) |  |  | 2006 |\n",
    "| [**Q-Learning**](https://example.com) | A model-free reinforcement learning algorithm to learn the value of an action in a particular state. | - Reinforcement Learning <br> | 1992 |\n",
    "| [**Actor Critic**](https://example.com) |  |  | early 21st century |\n",
    "| [**Affinity Propagation**](https://example.com) | Creates clusters by sending messages between pairs of samples. | - Unsupervised Learning <br> | 2007 |\n",
    "| [**t-Distributed Stochastic Neighbor Embedding (t-SNE)**](https://example.com) | A non-linear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization. | - Unsupervised Learning <br> | 2008 |\n",
    "| [**VAEs**](https://example.com) |  |  | 2010s |\n",
    "| [**Deep Q-Network (DQN)**](https://example.com) | Combines Q-learning with deep neural networks. | - Reinforcement Learning <br> | 2013 |\n",
    "| [**Extreme Gradient Boosting (XGBoost)**](14.%20Extreme%20Gradient%20Boosting%20(XGBoost)%20(Supervised%20Learning,%20Ensemble%20Method)/14.1%20XGBoost.ipynb) | A scalable and accurate implementation of gradient boosting machines. | - Supervised Learning <br> | 2014 |\n",
    "| [**Trust Region Policy Optimization (TRPO)**](https://example.com) | Maximizes a surrogate objective function using trust region methods. | - Reinforcement Learning <br> | 2015 |\n",
    "| [**U-Net**](https://example.com) | Used for fast and precise segmentation of images. | - Deep Learning <br> | 2015 |\n",
    "| [**Generative Adversarial Networks (GANs)**](https://example.com) | Consists of two networks, a generator and a discriminator, which contest with each other. | - Deep Learning <br> | 2014 |\n",
    "| [**GRU**](https://example.com) |  |  | 2014 |\n",
    "| [**YOLO (You Only Look Once)**](https://example.com) | A real-time object detection system. | - Deep Learning <br> | 2016 |\n",
    "| [**Proximal Policy Optimization (PPO)**](https://example.com) | A policy gradient method for reinforcement learning. | - Reinforcement Learning <br> | 2017 |\n",
    "| [**LightGBM**](15.%20LightGBM%20(Supervised%20Learning,%20Ensemble%20Method)/15.1%20Light%20GBM.ipynb) | A gradient boosting framework designed for speed and efficiency. | - Supervised Learning <br> | 2017 by Microsoft |\n",
    "| [**CatBoost**](16.%20CatBoost%20(Supervised%20Learning,%20Ensemble%20Method)/16.%20CatBoost.ipynb) | An algorithm that uses gradient boosting on decision trees, with support for categorical variables. | - Supervised Learning | 2017 |\n",
    "| [**Transformer Networks**](https://example.com) |  |  | 2017 |\n",
    "| [**Bidirectional Encoder Representations from Transformers (BERT)**](https://example.com) | Designed to understand the context of a word in a sentence, bidirectionally. | - Deep Learning <br> | 2018 |\n",
    "| [**UMap**](https://example.com) |  |  | 2018 |\n",
    "| [**GPT**](https://example.com) |  |  | 2018 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958f500-71c4-4a92-93b8-023baf0a8f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
