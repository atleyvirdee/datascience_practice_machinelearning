{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d9bd9a-d8b3-4c33-983c-3c3977e460e7",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques for Numerical Variables\n",
    "\n",
    "Preprocessing numerical variables is a critical step in the data preparation phase of a Machine Learning project. Here are various techniques used for this purpose:\n",
    "\n",
    "## 1. Normalization\n",
    "- **What It Is**: Scaling all numerical variables to a fixed range, typically between 0 and 1.\n",
    "- **How It's Done**: Divide each value by the maximum value in the dataset.\n",
    "- **When to Use**: When you have variables with different scales and need to bring them to a common scale without distorting differences in the ranges.\n",
    "- **Example**: Rescaling annual incomes in a dataset ranging from $30,000 to $100,000 to a scale of 0 to 1.\n",
    "- **Applied Technique**: Normalize by dividing each income by $100,000.\n",
    "\n",
    "### 1.1. Decimal Scaling Normalization \n",
    "- **What It Is**: Decimal scaling is a data normalization technique where we move the decimal point of values of a feature. This process changes the scale of the values but not the relationship between the values in the feature.\n",
    "- **How It's Done**: The number of decimal places moved depends on the maximum absolute value of the feature. If the maximum absolute value is a d-digit number, the data is divided by 10^d.\n",
    "- **When to Use**: To scale the feature values so that they fall between -1 and 1.\n",
    "- **Example**\n",
    "  **Data**: Consider a set of values [1234, 5678, 910].\n",
    "  **Calculation**: The maximum absolute value is 5678 (a 4-digit number). Therefore, we divide each value by 10^4.\n",
    "  **Result**: The scaled values will be [0.1234, 0.5678, 0.0910].\n",
    "\n",
    "### 1.2. Min-Max Normalization \n",
    "\n",
    "- **What It Is**: Min-Max Normalization is a common technique used in data preprocessing to scale numerical data.\n",
    "- **How It's Done**: This technique transforms features to a common scale by subtracting the minimum value of the feature and then dividing by the range of the feature (maximum value - minimum value).\n",
    "- **Formula**: \\[X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\]\n",
    "- **When to Use**: To scale the feature values so that they fall within a specified range, typically 0 to 1.\n",
    "- **Example**\n",
    "    **Data**: Consider a set of values [100, 200, 300].\n",
    "    **Calculation**: \n",
    "  - Minimum value, X_{min} = 100\n",
    "  - Maximum value, X_{max} = 300\n",
    "  - Normalized values = \\[(100-100)/(300-100), (200-100)/(300-100), (300-100)/(300-100)\\]\n",
    "    **Result**: The normalized values will be [0, 0.5, 1].\n",
    "\n",
    "This technique ensures that all features contribute equally to the result and helps in speeding up the convergence in algorithms that use gradient descent.\n",
    "\n",
    "\n",
    "## 2. Standardization\n",
    "- **What It Is**: Rescaling data to have a mean (μ) of 0 and a standard deviation (σ) of 1.\n",
    "- **How It's Done**: Subtract the mean and divide by the standard deviation.\n",
    "- **When to Use**: Useful in algorithms that assume data is normally distributed, like Support Vector Machines and k-Nearest Neighbors.\n",
    "- **Example**: Adjusting test scores from different classes to a common scale where the mean is 0 and the standard deviation is 1.\n",
    "- **Applied Technique**: Subtract the class mean from each score and divide by the class standard deviation.\n",
    "\n",
    "## 3. Log Transformation\n",
    "- **What It Is**: Applying the natural logarithm or logarithm to the data.\n",
    "- **How It's Done**: Replace each variable x with log(x).\n",
    "- **When to Use**: When dealing with skewed data or when you want to stabilize the variance.\n",
    "- **Example**: Applying log transformation to highly skewed real estate prices to reduce the impact of very high values.\n",
    "- **Applied Technique**: Apply the natural logarithm (log) to each price value.\n",
    "\n",
    "## 4. Clipping\n",
    "- **What It Is**: Capping the values in a dataset to a defined minimum or maximum.\n",
    "- **How It's Done**: Set thresholds and cap values exceeding these thresholds.\n",
    "- **When to Use**: To limit the effect of outliers that might skew the analysis.\n",
    "- **Example**: Capping credit card transactions at a certain threshold to minimize the impact of outliers.\n",
    "- **Applied Technique**: Replace all transactions above $10,000 with $10,000.\n",
    "\n",
    "## 5. Binning\n",
    "- **What It Is**: Converting continuous data into categorical by creating intervals.\n",
    "- **How It's Done**: Divide the range of the data into bins and assign each value to a bin.\n",
    "- **When to Use**: To simplify the model or when the exact value is not as important as the range.\n",
    "- **Example**: Converting continuous age data into categories like '18-30', '31-45', '46-60', etc.\n",
    "- **Applied Technique**: Define bins (18-30, 31-45, 46-60) and categorize each age into these bins.\n",
    "\n",
    "## 6. Feature Scaling\n",
    "- **What It Is**: Adjusting the scale of the features to a certain range.\n",
    "- **How It's Done**: Similar to normalization but can be done with different methods, like Min-Max scaling.\n",
    "- **When to Use**: When the algorithm is sensitive to the scale of the data, like Gradient Descent based algorithms.\n",
    "- **Example**: Scaling distance traveled and calories burned in a fitness app to a common scale for analysis.\n",
    "- **Applied Technique**: Use Min-Max scaling to bring both variables into a range of 0 to 1.\n",
    "\n",
    "## 7. Imputation\n",
    "- **What It Is**: The process of replacing missing values in a dataset.\n",
    "- **How It's Done**: Common methods include using the mean, median, or mode of the column, or more complex methods like using predictions from other parts of the data.\n",
    "- **When to Use**: Essential when you have missing data in your dataset. The choice of imputation method depends on the nature of the data and the extent of missingness.\n",
    "- **Example**: Filling in missing values in a survey dataset using the median or mean of the respective variable.\n",
    "- **Applied Technique**: Calculate the median of the variable and replace missing values with this median.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed51437-f965-419f-9310-2844fd1072fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Incomes: [ 30000  50000 100000]\n",
      "Normalized Incomes: [0.3 0.5 1. ]\n",
      "Original Test Scores: [70 80 90]\n",
      "Standardized Scores: [-1.22474487  0.          1.22474487]\n",
      "Original Real Estate Prices: [ 100000  200000 1000000]\n",
      "Log Transformed Prices: [11.51292546 12.20607265 13.81551056]\n",
      "Original Transactions: [ 5000 15000  8000]\n",
      "Clipped Transactions: [ 5000 10000  8000]\n",
      "Original Ages: [25 35 55]\n",
      "Binned Ages (Categories 1: <30, 2: 30-45, 3: 45-60): [1 2 3]\n",
      "Original Survey Data: [ 1. nan  3.  4.  5.]\n",
      "Imputed Survey Data (using median): [1.  3.5 3.  4.  5. ]\n"
     ]
    }
   ],
   "source": [
    "# Performing calculations for each preprocessing technique separately and preparing text outputs for each\n",
    "\n",
    "import numpy as np\n",
    "# 1. Normalization Example\n",
    "incomes = np.array([30000, 50000, 100000])\n",
    "normalized_incomes = incomes / 100000\n",
    "print( f\"Original Incomes: {incomes}\\nNormalized Incomes: {normalized_incomes}\")\n",
    "\n",
    "# 2. Standardization Example\n",
    "test_scores = np.array([70, 80, 90])\n",
    "standardized_scores = (test_scores - np.mean(test_scores)) / np.std(test_scores)\n",
    "print(f\"Original Test Scores: {test_scores}\\nStandardized Scores: {standardized_scores}\")\n",
    "\n",
    "# 3. Log Transformation Example\n",
    "real_estate_prices = np.array([100000, 200000, 1000000])\n",
    "log_transformed_prices = np.log(real_estate_prices)\n",
    "print(f\"Original Real Estate Prices: {real_estate_prices}\\nLog Transformed Prices: {log_transformed_prices}\")\n",
    "\n",
    "# 4. Clipping Example\n",
    "transactions = np.array([5000, 15000, 8000])\n",
    "clipped_transactions = np.clip(transactions, None, 10000)\n",
    "print(f\"Original Transactions: {transactions}\\nClipped Transactions: {clipped_transactions}\")\n",
    "\n",
    "# 5. Binning Example\n",
    "ages = np.array([25, 35, 55])\n",
    "age_bins = [0, 30, 45, 60]\n",
    "binned_ages = np.digitize(ages, age_bins)\n",
    "print( f\"Original Ages: {ages}\\nBinned Ages (Categories 1: <30, 2: 30-45, 3: 45-60): {binned_ages}\")\n",
    "\n",
    "# 6. Feature Scaling Example\n",
    "distance = np.array([5, 10, 15])  # in km\n",
    "calories = np.array([100, 200, 300])\n",
    "#scaled_distance = min_max_scaler(distance)\n",
    "#scaled_calories = min_max_scaler(calories)\n",
    "#feature_scaling_result = f\"Original Distance (km): {distance}\\nScaled Distance: {scaled_distance}\\nOriginal Calories: {calories}\\nScaled Calories: {scaled_calories}\"\n",
    "\n",
    "# 7. Imputation Example\n",
    "survey_data = np.array([1, np.nan, 3, 4, 5])\n",
    "median_value = np.nanmedian(survey_data)\n",
    "imputed_data = np.where(np.isnan(survey_data), median_value, survey_data)\n",
    "print( f\"Original Survey Data: {survey_data}\\nImputed Survey Data (using median): {imputed_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7703e-cbd8-46a6-81d2-4a9b65fb0d96",
   "metadata": {},
   "source": [
    "# Types of Missing Data\n",
    "\n",
    "Understanding the nature of missing data is crucial for effective data analysis and preprocessing. Here are the common types of missing data:\n",
    "\n",
    "## 1. Missing Completely at Random (MCAR)\n",
    "- **Characteristics**: The missingness is independent of any other data, both observed and unobserved. The reasons for the missing data are completely random.\n",
    "- **Implication**: Analysis is less likely to be biased because the missing data points do not represent a specific trend.\n",
    "- **Example**: A respondent accidentally skipping a question in a survey.\n",
    "\n",
    "## 2. Missing at Random (MAR)\n",
    "- **Characteristics**: The propensity for a data point to be missing is related to some observed data in the dataset, but not to the missing data itself.\n",
    "- **Implication**: The missing data can be handled by considering the observed data, as long as the relationship is correctly modeled.\n",
    "- **Example**: Younger people less likely to disclose their age in a survey, with age being the variable missing.\n",
    "\n",
    "## 3. Missing Not at Random (MNAR)\n",
    "- **Characteristics**: The missingness is related to the unobserved data, meaning the reason for the missing data is related to the missing data itself.\n",
    "- **Implication**: This type is challenging to handle as it can lead to biases if not addressed properly.\n",
    "- **Example**: Higher-income individuals less likely to disclose their income.\n",
    "\n",
    "## 4. Structurally Missing Data\n",
    "- **Characteristics**: The data is missing because it is not applicable or does not exist due to the nature of the data itself.\n",
    "- **Implication**: This type of missing data does not necessarily indicate a problem with the data collection process but reflects the nature of the data.\n",
    "- **Example**: The absence of a 'spouse's name' field for a single individual in a dataset.\n",
    "\n",
    "Each type of missing data has its implications on the analysis and requires specific strategies for handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e19718-4bdb-40be-812b-78c67fc15801",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques for Categorical Variables\n",
    "\n",
    "Categorical data requires specific preprocessing techniques to be effectively used in machine learning models. Here are some common methods:\n",
    "\n",
    "## 1. Label Encoding\n",
    "- **How It's Done**: Assign each unique category a numerical value.\n",
    "- **When to Use**: For ordinal data where categories have a meaningful order.\n",
    "- **Example**: 'Low', 'Medium', 'High' might be encoded as 1, 2, 3.\n",
    "- **Applied Technique**: Use Python's `LabelEncoder` from `sklearn.preprocessing` or a manual mapping.\n",
    "\n",
    "## 2. One-Hot Encoding\n",
    "- **How It's Done**: Create new columns indicating the presence of each category with binary values.\n",
    "- **When to Use**: For nominal data where no ordinal relationship exists.\n",
    "- **Example**: 'Red', 'Green', 'Blue' becomes three columns with 1s and 0s.\n",
    "- **Applied Technique**: Use `get_dummies` from Pandas or `OneHotEncoder` from `sklearn.preprocessing`.\n",
    "\n",
    "## 3. Binary Encoding\n",
    "- **How It's Done**: Convert categories to integers and then convert those integers into binary numbers.\n",
    "- **When to Use**: With high cardinality features to avoid dimensionality issues.\n",
    "- **Example**: Categories 1, 2, 3 might be encoded into 01, 10, 11.\n",
    "- **Applied Technique**: Use `BinaryEncoder` from the `category_encoders` library.\n",
    "\n",
    "## 4. Frequency or Count Encoding\n",
    "- **How It's Done**: Replace categories with their frequencies or count in the dataset.\n",
    "- **When to Use**: When the frequency of occurrence is informative.\n",
    "- **Example**: If 'Apple' appears 50 times, it's replaced with 50.\n",
    "- **Applied Technique**: Manually calculate frequencies or use `value_counts` in Pandas.\n",
    "\n",
    "## 5. Ordinal Encoding\n",
    "- **How It's Done**: Assign numbers to categories with respect for their order.\n",
    "- **When to Use**: For ordinal data with a known ranking.\n",
    "- **Example**: 'Bad', 'Good', 'Excellent' might be encoded as 1, 2, 3.\n",
    "- **Applied Technique**: Use `OrdinalEncoder` from `sklearn.preprocessing`.\n",
    "\n",
    "## 6. Mean Encoding (or Target Encoding)\n",
    "- **How It's Done**: Replace categories with the mean of the target variable for that category.\n",
    "- **When to Use**: When the category is strongly related to the target variable.\n",
    "- **Example**: Replace 'Car Brand' with the average sale price of each brand.\n",
    "- **Applied Technique**: Use `TargetEncoder` from the `category_encoders` library.\n",
    "\n",
    "## 7. Hashing\n",
    "- **How It's Done**: Use a hashing function to convert categories into numerical values.\n",
    "- **When to Use**: For large datasets and high-cardinality features.\n",
    "- **Example**: Hash 'City Name' into a fixed size of numerical values.\n",
    "- **Applied Technique**: Use `HashingEncoder` from the `category_encoders` library.\n",
    "\n",
    "## 8. Dummy Encoding\n",
    "- **How It's Done**: Similar to one-hot encoding but creates N-1 binary columns for N categories.\n",
    "- **When to Use**: To avoid multicollinearity in linear models.\n",
    "- **Example**: For 'Red', 'Green', 'Blue', create two columns instead of three.\n",
    "- **Applied Technique**: Use `pd.get_dummies(df, drop_first=True)` in Pandas.\n",
    "\n",
    "Each technique offers unique advantages and can be selected based on the model's requirements and the data's characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5890fc-635c-44a1-81f1-f0dd620b1e13",
   "metadata": {},
   "source": [
    "# Train-Test Split in Machine Learning\r\n",
    "\r\n",
    "The Train-Test Split is a fundamental step in the machine learning workflow, essential for evaluating model performance.\r\n",
    "\r\n",
    "## Overview\r\n",
    "- **Purpose**: To divide the dataset into two parts: one for training the model (training set) and one for testing the model's performance (test set).\r\n",
    "\r\n",
    "## Training Set\r\n",
    "- **Usage**: This subset is used to train or fit the machine learning model. It includes both the input data and the expected output (labels).\r\n",
    "- **Proportion**: Typically constitutes 70-80% of the dataset.\r\n",
    "\r\n",
    "## Test Set\r\n",
    "- **Usage**: Used for evaluating the performance of the model, ensuring that it can generalize well to new, unseen data.\r\n",
    "- **Proportion**: Commonly makes up the remaining 20-30% of the dataset.\r\n",
    "\r\n",
    "## Importance\r\n",
    "- **Model Generalization**: The separation into training and test sets allows for an unbiased evaluation of how well the model performs on new data.\r\n",
    "- **Prevention of Overfitting**: Helps in detecting overfitting, where the model performs well on the training data but poorly on unseen data.\r\n",
    "\r\n",
    "## Splitting Methods\r\n",
    "- **Random Splitting**: The data is divided randomly into training and test sets.\r\n",
    "- **Stratified Splitting**: Used especially for imbalanced datasets to maintain the same proportion of classes in both sets as in the original dataset.\r\n",
    "- **Cross-Validation**: Involves multiple train-test splits, providing a more robust model evaluation, especially for smaller datasets.\r\n",
    "\r\n",
    "Understanding and implementing the Train-Test Split correctly is crucial for the development of effective and reliable machine learning models.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459752d9-9e78-444e-b8dd-a585f4003c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
